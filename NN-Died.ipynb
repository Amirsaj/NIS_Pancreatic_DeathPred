{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4089c503-0bed-4983-9ad5-91193b6ba4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>9 hours 52 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Chicago</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.40.0.4</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 6 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_unknownUser_v2fhnr</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>25.57 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>30</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>30</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.4 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------\n",
       "H2O_cluster_uptime:         9 hours 52 mins\n",
       "H2O_cluster_timezone:       America/Chicago\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.40.0.4\n",
       "H2O_cluster_version_age:    2 months and 6 days\n",
       "H2O_cluster_name:           H2O_from_python_unknownUser_v2fhnr\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    25.57 Gb\n",
       "H2O_cluster_total_cores:    30\n",
       "H2O_cluster_allowed_cores:  30\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.4 final\n",
       "--------------------------  ----------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "import pandas as pd\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.estimators import H2ODeepLearningEstimator\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ae013-7800-4a8e-9b97-077c87eb7943",
   "metadata": {},
   "source": [
    "# Died Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "998d55f1-b86e-4cdd-9448-8fcc7bda8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('/raid-18/CHS/jakeluo/share/notebook/ataleban@uwm.edu/Panc_/X_Cleared_Balance1.5.csv')  \n",
    "Y = pd.read_csv('/raid-18/CHS/jakeluo/share/notebook/ataleban@uwm.edu/Panc_/Y_Cleared_Balance1.5.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df417d04-0719-44a1-9df6-2bfbeedf8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Unnamed: 0'], axis=1)\n",
    "X['died'] = Y['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15880918-c309-4836-8d6e-9d29fa88e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c765a53e-dd69-4aa9-9b02-ae9500602e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71c9dfc2-357d-4f99-a50e-4460e9da66cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "data = h2o.H2OFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61b50a3e-81a6-43d9-b1df-76c90da04abb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "race2.0\n",
      "race6.0\n",
      "race5.0\n",
      "race4.0\n",
      "race3.0\n",
      "race0.0\n",
      "race1.0\n",
      "female\n",
      "BLD001\n",
      "BLD002\n",
      "BLD003\n",
      "BLD004\n",
      "BLD005\n",
      "BLD006\n",
      "BLD007\n",
      "BLD008\n",
      "BLD009\n",
      "BLD010\n",
      "CIR001\n",
      "CIR002\n",
      "CIR003\n",
      "CIR004\n",
      "CIR005\n",
      "CIR006\n",
      "CIR007\n",
      "CIR008\n",
      "CIR009\n",
      "CIR010\n",
      "CIR011\n",
      "CIR012\n",
      "CIR013\n",
      "CIR014\n",
      "CIR015\n",
      "CIR016\n",
      "CIR017\n",
      "CIR018\n",
      "CIR019\n",
      "CIR020\n",
      "CIR021\n",
      "CIR022\n",
      "CIR023\n",
      "CIR024\n",
      "CIR025\n",
      "CIR026\n",
      "CIR027\n",
      "CIR028\n",
      "CIR029\n",
      "CIR030\n",
      "CIR031\n",
      "CIR032\n",
      "CIR033\n",
      "CIR034\n",
      "CIR035\n",
      "CIR036\n",
      "CIR038\n",
      "CIR039\n",
      "DIG001\n",
      "DIG002\n",
      "DIG003\n",
      "DIG004\n",
      "DIG005\n",
      "DIG006\n",
      "DIG007\n",
      "DIG008\n",
      "DIG009\n",
      "DIG010\n",
      "DIG011\n",
      "DIG012\n",
      "DIG013\n",
      "DIG014\n",
      "DIG015\n",
      "DIG016\n",
      "DIG017\n",
      "DIG018\n",
      "DIG019\n",
      "DIG020\n",
      "DIG021\n",
      "DIG022\n",
      "DIG023\n",
      "DIG024\n",
      "DIG025\n",
      "EAR001\n",
      "EAR002\n",
      "EAR003\n",
      "EAR004\n",
      "EAR006\n",
      "END001\n",
      "END002\n",
      "END003\n",
      "END007\n",
      "END008\n",
      "END009\n",
      "END010\n",
      "END011\n",
      "END012\n",
      "END013\n",
      "END014\n",
      "END015\n",
      "END016\n",
      "END017\n",
      "EYE001\n",
      "EYE002\n",
      "EYE003\n",
      "EYE004\n",
      "EYE005\n",
      "EYE006\n",
      "EYE007\n",
      "EYE008\n",
      "EYE009\n",
      "EYE010\n",
      "EYE012\n",
      "FAC003\n",
      "FAC004\n",
      "FAC006\n",
      "FAC008\n",
      "FAC009\n",
      "FAC014\n",
      "FAC016\n",
      "FAC020\n",
      "FAC021\n",
      "FAC022\n",
      "FAC025\n",
      "GEN001\n",
      "GEN002\n",
      "GEN003\n",
      "GEN004\n",
      "GEN005\n",
      "GEN006\n",
      "GEN007\n",
      "GEN008\n",
      "GEN009\n",
      "GEN010\n",
      "GEN012\n",
      "GEN013\n",
      "GEN014\n",
      "GEN016\n",
      "GEN017\n",
      "GEN018\n",
      "GEN019\n",
      "GEN020\n",
      "GEN021\n",
      "GEN022\n",
      "GEN023\n",
      "GEN025\n",
      "GEN026\n",
      "INF001\n",
      "INF002\n",
      "INF003\n",
      "INF004\n",
      "INF005\n",
      "INF006\n",
      "INF007\n",
      "INF008\n",
      "INF009\n",
      "INF010\n",
      "INF011\n",
      "INJ001\n",
      "INJ002\n",
      "INJ003\n",
      "INJ004\n",
      "INJ005\n",
      "INJ006\n",
      "INJ007\n",
      "INJ008\n",
      "INJ009\n",
      "INJ010\n",
      "INJ011\n",
      "INJ012\n",
      "INJ013\n",
      "INJ016\n",
      "INJ017\n",
      "INJ019\n",
      "INJ020\n",
      "INJ021\n",
      "INJ022\n",
      "INJ023\n",
      "INJ024\n",
      "INJ025\n",
      "INJ026\n",
      "INJ027\n",
      "INJ028\n",
      "INJ030\n",
      "INJ031\n",
      "INJ033\n",
      "INJ034\n",
      "INJ035\n",
      "INJ036\n",
      "INJ037\n",
      "INJ039\n",
      "INJ040\n",
      "INJ041\n",
      "INJ042\n",
      "INJ043\n",
      "INJ045\n",
      "INJ047\n",
      "INJ049\n",
      "INJ050\n",
      "INJ054\n",
      "INJ069\n",
      "INJ072\n",
      "INJ073\n",
      "INJ074\n",
      "INJ076\n",
      "MAL001\n",
      "MAL002\n",
      "MAL003\n",
      "MAL004\n",
      "MAL008\n",
      "MAL009\n",
      "MAL010\n",
      "MBD001\n",
      "MBD002\n",
      "MBD003\n",
      "MBD004\n",
      "MBD005\n",
      "MBD006\n",
      "MBD007\n",
      "MBD008\n",
      "MBD009\n",
      "MBD010\n",
      "MBD011\n",
      "MBD012\n",
      "MBD013\n",
      "MBD014\n",
      "MBD017\n",
      "MBD018\n",
      "MBD019\n",
      "MBD020\n",
      "MBD021\n",
      "MBD022\n",
      "MBD024\n",
      "MBD025\n",
      "MBD026\n",
      "MUS001\n",
      "MUS002\n",
      "MUS003\n",
      "MUS004\n",
      "MUS006\n",
      "MUS007\n",
      "MUS008\n",
      "MUS009\n",
      "MUS010\n",
      "MUS011\n",
      "MUS012\n",
      "MUS013\n",
      "MUS014\n",
      "MUS015\n",
      "MUS016\n",
      "MUS017\n",
      "MUS020\n",
      "MUS021\n",
      "MUS022\n",
      "MUS023\n",
      "MUS024\n",
      "MUS025\n",
      "MUS026\n",
      "MUS028\n",
      "MUS029\n",
      "MUS030\n",
      "MUS032\n",
      "MUS033\n",
      "MUS034\n",
      "MUS037\n",
      "MUS038\n",
      "NEO002\n",
      "NEO007\n",
      "NEO008\n",
      "NEO009\n",
      "NEO010\n",
      "NEO012\n",
      "NEO013\n",
      "NEO014\n",
      "NEO015\n",
      "NEO016\n",
      "NEO017\n",
      "NEO018\n",
      "NEO019\n",
      "NEO020\n",
      "NEO021\n",
      "NEO022\n",
      "NEO023\n",
      "NEO024\n",
      "NEO025\n",
      "NEO026\n",
      "NEO027\n",
      "NEO028\n",
      "NEO029\n",
      "NEO030\n",
      "NEO031\n",
      "NEO032\n",
      "NEO033\n",
      "NEO035\n",
      "NEO036\n",
      "NEO039\n",
      "NEO040\n",
      "NEO043\n",
      "NEO044\n",
      "NEO045\n",
      "NEO047\n",
      "NEO048\n",
      "NEO050\n",
      "NEO051\n",
      "NEO056\n",
      "NEO057\n",
      "NEO058\n",
      "NEO059\n",
      "NEO060\n",
      "NEO061\n",
      "NEO062\n",
      "NEO063\n",
      "NEO064\n",
      "NEO065\n",
      "NEO066\n",
      "NEO067\n",
      "NEO068\n",
      "NEO069\n",
      "NEO070\n",
      "NEO071\n",
      "NEO072\n",
      "NEO073\n",
      "NEO074\n",
      "NVS001\n",
      "NVS002\n",
      "NVS003\n",
      "NVS004\n",
      "NVS005\n",
      "NVS006\n",
      "NVS007\n",
      "NVS008\n",
      "NVS009\n",
      "NVS010\n",
      "NVS011\n",
      "NVS012\n",
      "NVS013\n",
      "NVS014\n",
      "NVS015\n",
      "NVS016\n",
      "NVS017\n",
      "NVS018\n",
      "NVS019\n",
      "NVS020\n",
      "NVS021\n",
      "NVS022\n",
      "PRG028\n",
      "RSP001\n",
      "RSP002\n",
      "RSP003\n",
      "RSP004\n",
      "RSP005\n",
      "RSP006\n",
      "RSP007\n",
      "RSP008\n",
      "RSP009\n",
      "RSP010\n",
      "RSP011\n",
      "RSP013\n",
      "RSP014\n",
      "RSP015\n",
      "RSP016\n",
      "RSP017\n",
      "SKN001\n",
      "SKN002\n",
      "SKN003\n",
      "SKN004\n",
      "SKN005\n",
      "SKN006\n",
      "SKN007\n",
      "SYM001\n",
      "SYM002\n",
      "SYM003\n",
      "SYM004\n",
      "SYM005\n",
      "SYM006\n",
      "SYM007\n",
      "SYM008\n",
      "SYM010\n",
      "SYM011\n",
      "SYM012\n",
      "SYM013\n",
      "SYM014\n",
      "SYM015\n",
      "SYM016\n",
      "SYM017\n",
      "XXX000\n",
      "ADM001\n",
      "ADM002\n",
      "ADM003\n",
      "ADM004\n",
      "ADM005\n",
      "ADM006\n",
      "ADM007\n",
      "ADM010\n",
      "ADM012\n",
      "ADM013\n",
      "ADM014\n",
      "ADM015\n",
      "ADM016\n",
      "ADM017\n",
      "ADM018\n",
      "ADM020\n",
      "ADM021\n",
      "CAR002\n",
      "CAR003\n",
      "CAR004\n",
      "CAR006\n",
      "CAR007\n",
      "CAR008\n",
      "CAR009\n",
      "CAR010\n",
      "CAR011\n",
      "CAR012\n",
      "CAR014\n",
      "CAR015\n",
      "CAR016\n",
      "CAR017\n",
      "CAR019\n",
      "CAR020\n",
      "CAR021\n",
      "CAR022\n",
      "CAR023\n",
      "CAR024\n",
      "CAR025\n",
      "CAR026\n",
      "CAR028\n",
      "CAR029\n",
      "CNS001\n",
      "CNS002\n",
      "CNS004\n",
      "CNS005\n",
      "CNS006\n",
      "CNS007\n",
      "CNS008\n",
      "CNS010\n",
      "CNS011\n",
      "CNS013\n",
      "CNS014\n",
      "ENP001\n",
      "ENP004\n",
      "ENT001\n",
      "ENT002\n",
      "ENT003\n",
      "ENT006\n",
      "ENT013\n",
      "ENT015\n",
      "ENT017\n",
      "ESA001\n",
      "ESA002\n",
      "ESA004\n",
      "ESA005\n",
      "ESA006\n",
      "ESA007\n",
      "ESA008\n",
      "ESA009\n",
      "ESA010\n",
      "EST001\n",
      "EST004\n",
      "EST005\n",
      "FRS001\n",
      "FRS002\n",
      "FRS003\n",
      "FRS004\n",
      "FRS010\n",
      "FRS015\n",
      "GIS001\n",
      "GIS002\n",
      "GIS003\n",
      "GIS004\n",
      "GIS005\n",
      "GIS006\n",
      "GIS007\n",
      "GIS008\n",
      "GIS009\n",
      "GIS010\n",
      "GIS011\n",
      "GIS012\n",
      "GIS013\n",
      "GIS014\n",
      "GIS015\n",
      "GIS016\n",
      "GIS017\n",
      "GIS018\n",
      "GIS019\n",
      "GIS021\n",
      "GIS022\n",
      "GIS024\n",
      "GIS025\n",
      "GIS026\n",
      "GIS027\n",
      "GIS028\n",
      "GIS029\n",
      "GNR002\n",
      "GNR003\n",
      "GNR004\n",
      "GNR005\n",
      "GNR006\n",
      "GNR007\n",
      "GNR008\n",
      "GNR009\n",
      "HEP001\n",
      "HEP002\n",
      "HEP003\n",
      "HEP004\n",
      "HEP005\n",
      "HEP006\n",
      "HEP007\n",
      "HEP008\n",
      "HEP009\n",
      "HEP010\n",
      "HEP013\n",
      "IMG001\n",
      "IMG002\n",
      "IMG003\n",
      "IMG004\n",
      "IMG005\n",
      "IMG006\n",
      "IMG007\n",
      "IMG008\n",
      "IMG009\n",
      "LYM001\n",
      "LYM002\n",
      "LYM003\n",
      "LYM005\n",
      "LYM006\n",
      "LYM007\n",
      "MAM001\n",
      "MAM002\n",
      "MAM003\n",
      "MAM004\n",
      "MAM005\n",
      "MAM006\n",
      "MAM007\n",
      "MAM008\n",
      "MAM009\n",
      "MAM010\n",
      "MAM011\n",
      "MAM013\n",
      "MAM015\n",
      "MHT002\n",
      "MHT003\n",
      "MRS003\n",
      "MRS004\n",
      "MRS005\n",
      "MST001\n",
      "MST002\n",
      "MST003\n",
      "MST004\n",
      "MST005\n",
      "MST006\n",
      "MST007\n",
      "MST008\n",
      "MST009\n",
      "MST010\n",
      "MST011\n",
      "MST012\n",
      "MST013\n",
      "MST014\n",
      "MST015\n",
      "MST016\n",
      "MST017\n",
      "MST018\n",
      "MST019\n",
      "MST020\n",
      "MST022\n",
      "MST023\n",
      "MST024\n",
      "MST025\n",
      "MST028\n",
      "MST029\n",
      "MST030\n",
      "NCM001\n",
      "NCM002\n",
      "OST001\n",
      "OTR001\n",
      "OTR002\n",
      "OTR004\n",
      "PLC001\n",
      "PLC002\n",
      "PNS001\n",
      "PNS002\n",
      "PNS003\n",
      "PNS005\n",
      "PNS006\n",
      "RAD001\n",
      "RAD002\n",
      "RAD003\n",
      "RAD004\n",
      "RES001\n",
      "RES002\n",
      "RES003\n",
      "RES004\n",
      "RES005\n",
      "RES006\n",
      "RES008\n",
      "RES009\n",
      "RES010\n",
      "RES011\n",
      "RES012\n",
      "RES014\n",
      "RHB001\n",
      "RHB002\n",
      "RHB003\n",
      "RHB004\n",
      "SKB001\n",
      "SKB002\n",
      "SKB006\n",
      "SKB007\n",
      "SKB008\n",
      "SKB009\n",
      "SKB010\n",
      "SUD001\n",
      "SUD002\n",
      "URN001\n",
      "URN002\n",
      "URN003\n",
      "URN004\n",
      "URN005\n",
      "URN006\n",
      "URN007\n",
      "URN008\n",
      "URN009\n",
      "URN010\n",
      "URN012\n"
     ]
    }
   ],
   "source": [
    "target = 'died'\n",
    "exclude = ['aweekend','amonth','drg_nopoa','hosp_division9.0','hosp_division1.0','hosp_division2.0','hosp_division3.0','hosp_division4.0','hosp_division5.0','hosp_division6.0','hosp_division7.0','hosp_division8.0','age_neonate','elective','mdc','mdc_nopoa','tran_in','zipinc_qrtl','pclass_orproc','pay15.0','pay10.0','pay16.0','pay13.0','pay11.0','year','los','hcup_ed2.0','hcup_ed0.0','i10_npr','totchg','pay14.0','pay12.0','hcup_ed1.0','pl_nchs','i10_necause','hcup_ed3.0','hcup_ed4.0',]\n",
    "features = []\n",
    "for i in col:\n",
    "    if i != target and i not in exclude:\n",
    "        features.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce677f2-2607-4c0d-adb9-c8dfaef618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target] = data[target].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1896ae86-0119-4b93-833f-03c9ba606391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = data.split_frame(ratios=[0.8])\n",
    "train_ratio, valid_ratio, test_ratio = 0.7, 0.1, 0.2\n",
    "# train_ratio, test_ratio = 0.7, 0.3\n",
    "train, valid, test = data.split_frame(ratios=[train_ratio, valid_ratio], seed=42)\n",
    "\n",
    "# train, test ,valid= data.split_frame(ratios=[train_ratio,valid_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ff0495-17fd-4eec-a08d-36b1d6ce44b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'race2.0',\n",
       " 'race6.0',\n",
       " 'race5.0',\n",
       " 'race4.0',\n",
       " 'race3.0',\n",
       " 'race0.0',\n",
       " 'race1.0',\n",
       " 'female',\n",
       " 'BLD001',\n",
       " 'BLD002',\n",
       " 'BLD003',\n",
       " 'BLD004',\n",
       " 'BLD005',\n",
       " 'BLD006',\n",
       " 'BLD007',\n",
       " 'BLD008',\n",
       " 'BLD009',\n",
       " 'BLD010',\n",
       " 'CIR001',\n",
       " 'CIR002',\n",
       " 'CIR003',\n",
       " 'CIR004',\n",
       " 'CIR005',\n",
       " 'CIR006',\n",
       " 'CIR007',\n",
       " 'CIR008',\n",
       " 'CIR009',\n",
       " 'CIR010',\n",
       " 'CIR011',\n",
       " 'CIR012',\n",
       " 'CIR013',\n",
       " 'CIR014',\n",
       " 'CIR015',\n",
       " 'CIR016',\n",
       " 'CIR017',\n",
       " 'CIR018',\n",
       " 'CIR019',\n",
       " 'CIR020',\n",
       " 'CIR021',\n",
       " 'CIR022',\n",
       " 'CIR023',\n",
       " 'CIR024',\n",
       " 'CIR025',\n",
       " 'CIR026',\n",
       " 'CIR027',\n",
       " 'CIR028',\n",
       " 'CIR029',\n",
       " 'CIR030',\n",
       " 'CIR031',\n",
       " 'CIR032',\n",
       " 'CIR033',\n",
       " 'CIR034',\n",
       " 'CIR035',\n",
       " 'CIR036',\n",
       " 'CIR038',\n",
       " 'CIR039',\n",
       " 'DIG001',\n",
       " 'DIG002',\n",
       " 'DIG003',\n",
       " 'DIG004',\n",
       " 'DIG005',\n",
       " 'DIG006',\n",
       " 'DIG007',\n",
       " 'DIG008',\n",
       " 'DIG009',\n",
       " 'DIG010',\n",
       " 'DIG011',\n",
       " 'DIG012',\n",
       " 'DIG013',\n",
       " 'DIG014',\n",
       " 'DIG015',\n",
       " 'DIG016',\n",
       " 'DIG017',\n",
       " 'DIG018',\n",
       " 'DIG019',\n",
       " 'DIG020',\n",
       " 'DIG021',\n",
       " 'DIG022',\n",
       " 'DIG023',\n",
       " 'DIG024',\n",
       " 'DIG025',\n",
       " 'EAR001',\n",
       " 'EAR002',\n",
       " 'EAR003',\n",
       " 'EAR004',\n",
       " 'EAR006',\n",
       " 'END001',\n",
       " 'END002',\n",
       " 'END003',\n",
       " 'END007',\n",
       " 'END008',\n",
       " 'END009',\n",
       " 'END010',\n",
       " 'END011',\n",
       " 'END012',\n",
       " 'END013',\n",
       " 'END014',\n",
       " 'END015',\n",
       " 'END016',\n",
       " 'END017',\n",
       " 'EYE001',\n",
       " 'EYE002',\n",
       " 'EYE003',\n",
       " 'EYE004',\n",
       " 'EYE005',\n",
       " 'EYE006',\n",
       " 'EYE007',\n",
       " 'EYE008',\n",
       " 'EYE009',\n",
       " 'EYE010',\n",
       " 'EYE012',\n",
       " 'FAC003',\n",
       " 'FAC004',\n",
       " 'FAC006',\n",
       " 'FAC008',\n",
       " 'FAC009',\n",
       " 'FAC014',\n",
       " 'FAC016',\n",
       " 'FAC020',\n",
       " 'FAC021',\n",
       " 'FAC022',\n",
       " 'FAC025',\n",
       " 'GEN001',\n",
       " 'GEN002',\n",
       " 'GEN003',\n",
       " 'GEN004',\n",
       " 'GEN005',\n",
       " 'GEN006',\n",
       " 'GEN007',\n",
       " 'GEN008',\n",
       " 'GEN009',\n",
       " 'GEN010',\n",
       " 'GEN012',\n",
       " 'GEN013',\n",
       " 'GEN014',\n",
       " 'GEN016',\n",
       " 'GEN017',\n",
       " 'GEN018',\n",
       " 'GEN019',\n",
       " 'GEN020',\n",
       " 'GEN021',\n",
       " 'GEN022',\n",
       " 'GEN023',\n",
       " 'GEN025',\n",
       " 'GEN026',\n",
       " 'INF001',\n",
       " 'INF002',\n",
       " 'INF003',\n",
       " 'INF004',\n",
       " 'INF005',\n",
       " 'INF006',\n",
       " 'INF007',\n",
       " 'INF008',\n",
       " 'INF009',\n",
       " 'INF010',\n",
       " 'INF011',\n",
       " 'INJ001',\n",
       " 'INJ002',\n",
       " 'INJ003',\n",
       " 'INJ004',\n",
       " 'INJ005',\n",
       " 'INJ006',\n",
       " 'INJ007',\n",
       " 'INJ008',\n",
       " 'INJ009',\n",
       " 'INJ010',\n",
       " 'INJ011',\n",
       " 'INJ012',\n",
       " 'INJ013',\n",
       " 'INJ016',\n",
       " 'INJ017',\n",
       " 'INJ019',\n",
       " 'INJ020',\n",
       " 'INJ021',\n",
       " 'INJ022',\n",
       " 'INJ023',\n",
       " 'INJ024',\n",
       " 'INJ025',\n",
       " 'INJ026',\n",
       " 'INJ027',\n",
       " 'INJ028',\n",
       " 'INJ030',\n",
       " 'INJ031',\n",
       " 'INJ033',\n",
       " 'INJ034',\n",
       " 'INJ035',\n",
       " 'INJ036',\n",
       " 'INJ037',\n",
       " 'INJ039',\n",
       " 'INJ040',\n",
       " 'INJ041',\n",
       " 'INJ042',\n",
       " 'INJ043',\n",
       " 'INJ045',\n",
       " 'INJ047',\n",
       " 'INJ049',\n",
       " 'INJ050',\n",
       " 'INJ054',\n",
       " 'INJ069',\n",
       " 'INJ072',\n",
       " 'INJ073',\n",
       " 'INJ074',\n",
       " 'INJ076',\n",
       " 'MAL001',\n",
       " 'MAL002',\n",
       " 'MAL003',\n",
       " 'MAL004',\n",
       " 'MAL008',\n",
       " 'MAL009',\n",
       " 'MAL010',\n",
       " 'MBD001',\n",
       " 'MBD002',\n",
       " 'MBD003',\n",
       " 'MBD004',\n",
       " 'MBD005',\n",
       " 'MBD006',\n",
       " 'MBD007',\n",
       " 'MBD008',\n",
       " 'MBD009',\n",
       " 'MBD010',\n",
       " 'MBD011',\n",
       " 'MBD012',\n",
       " 'MBD013',\n",
       " 'MBD014',\n",
       " 'MBD017',\n",
       " 'MBD018',\n",
       " 'MBD019',\n",
       " 'MBD020',\n",
       " 'MBD021',\n",
       " 'MBD022',\n",
       " 'MBD024',\n",
       " 'MBD025',\n",
       " 'MBD026',\n",
       " 'MUS001',\n",
       " 'MUS002',\n",
       " 'MUS003',\n",
       " 'MUS004',\n",
       " 'MUS006',\n",
       " 'MUS007',\n",
       " 'MUS008',\n",
       " 'MUS009',\n",
       " 'MUS010',\n",
       " 'MUS011',\n",
       " 'MUS012',\n",
       " 'MUS013',\n",
       " 'MUS014',\n",
       " 'MUS015',\n",
       " 'MUS016',\n",
       " 'MUS017',\n",
       " 'MUS020',\n",
       " 'MUS021',\n",
       " 'MUS022',\n",
       " 'MUS023',\n",
       " 'MUS024',\n",
       " 'MUS025',\n",
       " 'MUS026',\n",
       " 'MUS028',\n",
       " 'MUS029',\n",
       " 'MUS030',\n",
       " 'MUS032',\n",
       " 'MUS033',\n",
       " 'MUS034',\n",
       " 'MUS037',\n",
       " 'MUS038',\n",
       " 'NEO002',\n",
       " 'NEO007',\n",
       " 'NEO008',\n",
       " 'NEO009',\n",
       " 'NEO010',\n",
       " 'NEO012',\n",
       " 'NEO013',\n",
       " 'NEO014',\n",
       " 'NEO015',\n",
       " 'NEO016',\n",
       " 'NEO017',\n",
       " 'NEO018',\n",
       " 'NEO019',\n",
       " 'NEO020',\n",
       " 'NEO021',\n",
       " 'NEO022',\n",
       " 'NEO023',\n",
       " 'NEO024',\n",
       " 'NEO025',\n",
       " 'NEO026',\n",
       " 'NEO027',\n",
       " 'NEO028',\n",
       " 'NEO029',\n",
       " 'NEO030',\n",
       " 'NEO031',\n",
       " 'NEO032',\n",
       " 'NEO033',\n",
       " 'NEO035',\n",
       " 'NEO036',\n",
       " 'NEO039',\n",
       " 'NEO040',\n",
       " 'NEO043',\n",
       " 'NEO044',\n",
       " 'NEO045',\n",
       " 'NEO047',\n",
       " 'NEO048',\n",
       " 'NEO050',\n",
       " 'NEO051',\n",
       " 'NEO056',\n",
       " 'NEO057',\n",
       " 'NEO058',\n",
       " 'NEO059',\n",
       " 'NEO060',\n",
       " 'NEO061',\n",
       " 'NEO062',\n",
       " 'NEO063',\n",
       " 'NEO064',\n",
       " 'NEO065',\n",
       " 'NEO066',\n",
       " 'NEO067',\n",
       " 'NEO068',\n",
       " 'NEO069',\n",
       " 'NEO070',\n",
       " 'NEO071',\n",
       " 'NEO072',\n",
       " 'NEO073',\n",
       " 'NEO074',\n",
       " 'NVS001',\n",
       " 'NVS002',\n",
       " 'NVS003',\n",
       " 'NVS004',\n",
       " 'NVS005',\n",
       " 'NVS006',\n",
       " 'NVS007',\n",
       " 'NVS008',\n",
       " 'NVS009',\n",
       " 'NVS010',\n",
       " 'NVS011',\n",
       " 'NVS012',\n",
       " 'NVS013',\n",
       " 'NVS014',\n",
       " 'NVS015',\n",
       " 'NVS016',\n",
       " 'NVS017',\n",
       " 'NVS018',\n",
       " 'NVS019',\n",
       " 'NVS020',\n",
       " 'NVS021',\n",
       " 'NVS022',\n",
       " 'PRG028',\n",
       " 'RSP001',\n",
       " 'RSP002',\n",
       " 'RSP003',\n",
       " 'RSP004',\n",
       " 'RSP005',\n",
       " 'RSP006',\n",
       " 'RSP007',\n",
       " 'RSP008',\n",
       " 'RSP009',\n",
       " 'RSP010',\n",
       " 'RSP011',\n",
       " 'RSP013',\n",
       " 'RSP014',\n",
       " 'RSP015',\n",
       " 'RSP016',\n",
       " 'RSP017',\n",
       " 'SKN001',\n",
       " 'SKN002',\n",
       " 'SKN003',\n",
       " 'SKN004',\n",
       " 'SKN005',\n",
       " 'SKN006',\n",
       " 'SKN007',\n",
       " 'SYM001',\n",
       " 'SYM002',\n",
       " 'SYM003',\n",
       " 'SYM004',\n",
       " 'SYM005',\n",
       " 'SYM006',\n",
       " 'SYM007',\n",
       " 'SYM008',\n",
       " 'SYM010',\n",
       " 'SYM011',\n",
       " 'SYM012',\n",
       " 'SYM013',\n",
       " 'SYM014',\n",
       " 'SYM015',\n",
       " 'SYM016',\n",
       " 'SYM017',\n",
       " 'XXX000',\n",
       " 'ADM001',\n",
       " 'ADM002',\n",
       " 'ADM003',\n",
       " 'ADM004',\n",
       " 'ADM005',\n",
       " 'ADM006',\n",
       " 'ADM007',\n",
       " 'ADM010',\n",
       " 'ADM012',\n",
       " 'ADM013',\n",
       " 'ADM014',\n",
       " 'ADM015',\n",
       " 'ADM016',\n",
       " 'ADM017',\n",
       " 'ADM018',\n",
       " 'ADM020',\n",
       " 'ADM021',\n",
       " 'CAR002',\n",
       " 'CAR003',\n",
       " 'CAR004',\n",
       " 'CAR006',\n",
       " 'CAR007',\n",
       " 'CAR008',\n",
       " 'CAR009',\n",
       " 'CAR010',\n",
       " 'CAR011',\n",
       " 'CAR012',\n",
       " 'CAR014',\n",
       " 'CAR015',\n",
       " 'CAR016',\n",
       " 'CAR017',\n",
       " 'CAR019',\n",
       " 'CAR020',\n",
       " 'CAR021',\n",
       " 'CAR022',\n",
       " 'CAR023',\n",
       " 'CAR024',\n",
       " 'CAR025',\n",
       " 'CAR026',\n",
       " 'CAR028',\n",
       " 'CAR029',\n",
       " 'CNS001',\n",
       " 'CNS002',\n",
       " 'CNS004',\n",
       " 'CNS005',\n",
       " 'CNS006',\n",
       " 'CNS007',\n",
       " 'CNS008',\n",
       " 'CNS010',\n",
       " 'CNS011',\n",
       " 'CNS013',\n",
       " 'CNS014',\n",
       " 'ENP001',\n",
       " 'ENP004',\n",
       " 'ENT001',\n",
       " 'ENT002',\n",
       " 'ENT003',\n",
       " 'ENT006',\n",
       " 'ENT013',\n",
       " 'ENT015',\n",
       " 'ENT017',\n",
       " 'ESA001',\n",
       " 'ESA002',\n",
       " 'ESA004',\n",
       " 'ESA005',\n",
       " 'ESA006',\n",
       " 'ESA007',\n",
       " 'ESA008',\n",
       " 'ESA009',\n",
       " 'ESA010',\n",
       " 'EST001',\n",
       " 'EST004',\n",
       " 'EST005',\n",
       " 'FRS001',\n",
       " 'FRS002',\n",
       " 'FRS003',\n",
       " 'FRS004',\n",
       " 'FRS010',\n",
       " 'FRS015',\n",
       " 'GIS001',\n",
       " 'GIS002',\n",
       " 'GIS003',\n",
       " 'GIS004',\n",
       " 'GIS005',\n",
       " 'GIS006',\n",
       " 'GIS007',\n",
       " 'GIS008',\n",
       " 'GIS009',\n",
       " 'GIS010',\n",
       " 'GIS011',\n",
       " 'GIS012',\n",
       " 'GIS013',\n",
       " 'GIS014',\n",
       " 'GIS015',\n",
       " 'GIS016',\n",
       " 'GIS017',\n",
       " 'GIS018',\n",
       " 'GIS019',\n",
       " 'GIS021',\n",
       " 'GIS022',\n",
       " 'GIS024',\n",
       " 'GIS025',\n",
       " 'GIS026',\n",
       " 'GIS027',\n",
       " 'GIS028',\n",
       " 'GIS029',\n",
       " 'GNR002',\n",
       " 'GNR003',\n",
       " 'GNR004',\n",
       " 'GNR005',\n",
       " 'GNR006',\n",
       " 'GNR007',\n",
       " 'GNR008',\n",
       " 'GNR009',\n",
       " 'HEP001',\n",
       " 'HEP002',\n",
       " 'HEP003',\n",
       " 'HEP004',\n",
       " 'HEP005',\n",
       " 'HEP006',\n",
       " 'HEP007',\n",
       " 'HEP008',\n",
       " 'HEP009',\n",
       " 'HEP010',\n",
       " 'HEP013',\n",
       " 'IMG001',\n",
       " 'IMG002',\n",
       " 'IMG003',\n",
       " 'IMG004',\n",
       " 'IMG005',\n",
       " 'IMG006',\n",
       " 'IMG007',\n",
       " 'IMG008',\n",
       " 'IMG009',\n",
       " 'LYM001',\n",
       " 'LYM002',\n",
       " 'LYM003',\n",
       " 'LYM005',\n",
       " 'LYM006',\n",
       " 'LYM007',\n",
       " 'MAM001',\n",
       " 'MAM002',\n",
       " 'MAM003',\n",
       " 'MAM004',\n",
       " 'MAM005',\n",
       " 'MAM006',\n",
       " 'MAM007',\n",
       " 'MAM008',\n",
       " 'MAM009',\n",
       " 'MAM010',\n",
       " 'MAM011',\n",
       " 'MAM013',\n",
       " 'MAM015',\n",
       " 'MHT002',\n",
       " 'MHT003',\n",
       " 'MRS003',\n",
       " 'MRS004',\n",
       " 'MRS005',\n",
       " 'MST001',\n",
       " 'MST002',\n",
       " 'MST003',\n",
       " 'MST004',\n",
       " 'MST005',\n",
       " 'MST006',\n",
       " 'MST007',\n",
       " 'MST008',\n",
       " 'MST009',\n",
       " 'MST010',\n",
       " 'MST011',\n",
       " 'MST012',\n",
       " 'MST013',\n",
       " 'MST014',\n",
       " 'MST015',\n",
       " 'MST016',\n",
       " 'MST017',\n",
       " 'MST018',\n",
       " 'MST019',\n",
       " 'MST020',\n",
       " 'MST022',\n",
       " 'MST023',\n",
       " 'MST024',\n",
       " 'MST025',\n",
       " 'MST028',\n",
       " 'MST029',\n",
       " 'MST030',\n",
       " 'NCM001',\n",
       " 'NCM002',\n",
       " 'OST001',\n",
       " 'OTR001',\n",
       " 'OTR002',\n",
       " 'OTR004',\n",
       " 'PLC001',\n",
       " 'PLC002',\n",
       " 'PNS001',\n",
       " 'PNS002',\n",
       " 'PNS003',\n",
       " 'PNS005',\n",
       " 'PNS006',\n",
       " 'RAD001',\n",
       " 'RAD002',\n",
       " 'RAD003',\n",
       " 'RAD004',\n",
       " 'RES001',\n",
       " 'RES002',\n",
       " 'RES003',\n",
       " 'RES004',\n",
       " 'RES005',\n",
       " 'RES006',\n",
       " 'RES008',\n",
       " 'RES009',\n",
       " 'RES010',\n",
       " 'RES011',\n",
       " 'RES012',\n",
       " 'RES014',\n",
       " 'RHB001',\n",
       " 'RHB002',\n",
       " 'RHB003',\n",
       " 'RHB004',\n",
       " 'SKB001',\n",
       " 'SKB002',\n",
       " 'SKB006',\n",
       " 'SKB007',\n",
       " 'SKB008',\n",
       " 'SKB009',\n",
       " 'SKB010',\n",
       " 'SUD001',\n",
       " 'SUD002',\n",
       " 'URN001',\n",
       " 'URN002',\n",
       " 'URN003',\n",
       " 'URN004',\n",
       " 'URN005',\n",
       " 'URN006',\n",
       " 'URN007',\n",
       " 'URN008',\n",
       " 'URN009',\n",
       " 'URN010',\n",
       " 'URN012']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156df0e6-b212-4b4a-81b5-d16cc2c8d41b",
   "metadata": {},
   "source": [
    "# Traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10a4e3f9-528f-4959-9a05-11d0e40a793b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |\n",
      "13:05:09.265: _train param, Dropping bad and constant columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "█\n",
      "13:05:14.7: _train param, Dropping bad and constant columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "████\n",
      "13:05:28.82: _train param, Dropping bad and constant columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "█\n",
      "13:05:51.288: _train param, Dropping bad and constant columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "█\n",
      "13:05:59.857: _train param, Dropping bad and constant columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "██\n",
      "13:06:18.887: _train param, Dropping unused columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "██\n",
      "13:06:23.259: _train param, Dropping unused columns: [RSP004, EYE007, EAR001, hcup_ed2.0, NEO009, PNS003, MUS004, NEO036, GIS014, END017, INF011, URN012, INJ076, ENT015, NEO051, RHB003, age_neonate, MRS004, MRS003, INF001, URN007, NEO048, MHT003, PRG028, ENT006, MHT002]\n",
      "\n",
      "████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_1_AutoML_4_20230502_130509\n",
       "</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-10.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-10 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-10 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-10 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-10 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-10 .h2o-table th,\n",
       "#h2o-table-10 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Model Summary for Stacked Ensemble: </caption>\n",
       "    <thead><tr><th>key</th>\n",
       "<th>value</th></tr></thead>\n",
       "    <tbody><tr><td>Stacking strategy</td>\n",
       "<td>cross_validation</td></tr>\n",
       "<tr><td>Number of base models (used / total)</td>\n",
       "<td>5/5</td></tr>\n",
       "<tr><td># GBM base models (used / total)</td>\n",
       "<td>1/1</td></tr>\n",
       "<tr><td># XGBoost base models (used / total)</td>\n",
       "<td>2/2</td></tr>\n",
       "<tr><td># DRF base models (used / total)</td>\n",
       "<td>1/1</td></tr>\n",
       "<tr><td># GLM base models (used / total)</td>\n",
       "<td>1/1</td></tr>\n",
       "<tr><td>Metalearner algorithm</td>\n",
       "<td>GLM</td></tr>\n",
       "<tr><td>Metalearner fold assignment scheme</td>\n",
       "<td>Random</td></tr>\n",
       "<tr><td>Metalearner nfolds</td>\n",
       "<td>5</td></tr>\n",
       "<tr><td>Metalearner fold_column</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Custom metalearner hyperparameters</td>\n",
       "<td>None</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.018577308498790433\n",
       "RMSE: 0.13629860050195097\n",
       "LogLoss: 0.07196636886804604\n",
       "AUC: 0.9979130824487391\n",
       "AUCPR: 0.9972344208163098\n",
       "Gini: 0.9958261648974782\n",
       "Null degrees of freedom: 10042\n",
       "Residual degrees of freedom: 10037\n",
       "Null deviance: 13528.444734878416\n",
       "Residual deviance: 1445.5164850835727\n",
       "AIC: 1457.5164850835727</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-11.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-11 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-11 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-11 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-11 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-11 .h2o-table th,\n",
       "#h2o-table-11 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3731470151993044</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>0</th>\n",
       "<th>1</th>\n",
       "<th>Error</th>\n",
       "<th>Rate</th></tr></thead>\n",
       "    <tbody><tr><td>0</td>\n",
       "<td>5935.0</td>\n",
       "<td>78.0</td>\n",
       "<td>0.013</td>\n",
       "<td> (78.0/6013.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>119.0</td>\n",
       "<td>3911.0</td>\n",
       "<td>0.0295</td>\n",
       "<td> (119.0/4030.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>6054.0</td>\n",
       "<td>3989.0</td>\n",
       "<td>0.0196</td>\n",
       "<td> (197.0/10043.0)</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-12.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-12 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-12 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-12 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-12 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-12 .h2o-table th,\n",
       "#h2o-table-12 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
       "    <thead><tr><th>metric</th>\n",
       "<th>threshold</th>\n",
       "<th>value</th>\n",
       "<th>idx</th></tr></thead>\n",
       "    <tbody><tr><td>max f1</td>\n",
       "<td>0.3731470</td>\n",
       "<td>0.9754333</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2154515</td>\n",
       "<td>0.9775452</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4833056</td>\n",
       "<td>0.9835227</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3731470</td>\n",
       "<td>0.9803843</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999225</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0209459</td>\n",
       "<td>1.0</td>\n",
       "<td>363.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999225</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3731470</td>\n",
       "<td>0.9591433</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2956484</td>\n",
       "<td>0.9770497</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3731470</td>\n",
       "<td>0.9787498</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9999225</td>\n",
       "<td>6013.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9999225</td>\n",
       "<td>3032.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0011129</td>\n",
       "<td>6013.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0209459</td>\n",
       "<td>4030.0</td>\n",
       "<td>363.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9999225</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9999225</td>\n",
       "<td>0.7523573</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0011129</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0209459</td>\n",
       "<td>1.0</td>\n",
       "<td>363.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-13.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-13 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-13 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-13 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-13 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-13 .h2o-table th,\n",
       "#h2o-table-13 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Gains/Lift Table: Avg response rate: 40.13 %, avg score: 40.35 %</caption>\n",
       "    <thead><tr><th>group</th>\n",
       "<th>cumulative_data_fraction</th>\n",
       "<th>lower_threshold</th>\n",
       "<th>lift</th>\n",
       "<th>cumulative_lift</th>\n",
       "<th>response_rate</th>\n",
       "<th>score</th>\n",
       "<th>cumulative_response_rate</th>\n",
       "<th>cumulative_score</th>\n",
       "<th>capture_rate</th>\n",
       "<th>cumulative_capture_rate</th>\n",
       "<th>gain</th>\n",
       "<th>cumulative_gain</th>\n",
       "<th>kolmogorov_smirnov</th></tr></thead>\n",
       "    <tbody><tr><td>1</td>\n",
       "<td>0.0100568</td>\n",
       "<td>0.9999844</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999934</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999934</td>\n",
       "<td>0.0250620</td>\n",
       "<td>0.0250620</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.0250620</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200139</td>\n",
       "<td>0.9999702</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999776</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999855</td>\n",
       "<td>0.0248139</td>\n",
       "<td>0.0498759</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.0498759</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300707</td>\n",
       "<td>0.9999580</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999640</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999784</td>\n",
       "<td>0.0250620</td>\n",
       "<td>0.0749380</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.0749380</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400279</td>\n",
       "<td>0.9999442</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999509</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999715</td>\n",
       "<td>0.0248139</td>\n",
       "<td>0.0997519</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.0997519</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500846</td>\n",
       "<td>0.9999285</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999371</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999646</td>\n",
       "<td>0.0250620</td>\n",
       "<td>0.1248139</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.1248139</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000697</td>\n",
       "<td>0.9998195</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998799</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999223</td>\n",
       "<td>0.1245658</td>\n",
       "<td>0.2493797</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.2493797</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500548</td>\n",
       "<td>0.9996338</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997328</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998592</td>\n",
       "<td>0.1245658</td>\n",
       "<td>0.3739454</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.3739454</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000398</td>\n",
       "<td>0.9992961</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994787</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997641</td>\n",
       "<td>0.1245658</td>\n",
       "<td>0.4985112</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.4985112</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3000100</td>\n",
       "<td>0.9964554</td>\n",
       "<td>2.4920596</td>\n",
       "<td>2.4920596</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9983948</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9993078</td>\n",
       "<td>0.2491315</td>\n",
       "<td>0.7476427</td>\n",
       "<td>149.2059553</td>\n",
       "<td>149.2059553</td>\n",
       "<td>0.7476427</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.3999801</td>\n",
       "<td>0.3473125</td>\n",
       "<td>2.2512928</td>\n",
       "<td>2.4318829</td>\n",
       "<td>0.9033865</td>\n",
       "<td>0.7798946</td>\n",
       "<td>0.9758526</td>\n",
       "<td>0.9444682</td>\n",
       "<td>0.2250620</td>\n",
       "<td>0.9727047</td>\n",
       "<td>125.1292844</td>\n",
       "<td>143.1882860</td>\n",
       "<td>0.9565730</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5000498</td>\n",
       "<td>0.0771716</td>\n",
       "<td>0.2554051</td>\n",
       "<td>1.9963273</td>\n",
       "<td>0.1024876</td>\n",
       "<td>0.1560451</td>\n",
       "<td>0.8010753</td>\n",
       "<td>0.7866894</td>\n",
       "<td>0.0255583</td>\n",
       "<td>0.9982630</td>\n",
       "<td>-74.4594892</td>\n",
       "<td>99.6327277</td>\n",
       "<td>0.8321230</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.6000199</td>\n",
       "<td>0.0349229</td>\n",
       "<td>0.0148928</td>\n",
       "<td>1.6661978</td>\n",
       "<td>0.0059761</td>\n",
       "<td>0.0521331</td>\n",
       "<td>0.6686027</td>\n",
       "<td>0.6643040</td>\n",
       "<td>0.0014888</td>\n",
       "<td>0.9997519</td>\n",
       "<td>-98.5107214</td>\n",
       "<td>66.6197800</td>\n",
       "<td>0.6676381</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.6999900</td>\n",
       "<td>0.0180587</td>\n",
       "<td>0.0024821</td>\n",
       "<td>1.4285917</td>\n",
       "<td>0.0009960</td>\n",
       "<td>0.0256215</td>\n",
       "<td>0.5732575</td>\n",
       "<td>0.5730896</td>\n",
       "<td>0.0002481</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.7517869</td>\n",
       "<td>42.8591750</td>\n",
       "<td>0.5010810</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.7999602</td>\n",
       "<td>0.0097499</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500622</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0134106</td>\n",
       "<td>0.5016181</td>\n",
       "<td>0.5031471</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0062235</td>\n",
       "<td>0.3341094</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999303</td>\n",
       "<td>0.0048137</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111972</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0071142</td>\n",
       "<td>0.4458951</td>\n",
       "<td>0.4480445</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1197168</td>\n",
       "<td>0.1671379</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0006373</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0029099</td>\n",
       "<td>0.4012745</td>\n",
       "<td>0.4035000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 0.041538956172924135\n",
       "RMSE: 0.2038110796127731\n",
       "LogLoss: 0.14223209418142302\n",
       "AUC: 0.9830456110265015\n",
       "AUCPR: 0.9810762557763709\n",
       "Gini: 0.9660912220530029\n",
       "Null degrees of freedom: 12858\n",
       "Residual degrees of freedom: 12853\n",
       "Null deviance: 17318.11230397557\n",
       "Residual deviance: 3657.924998157837\n",
       "AIC: 3669.924998157837</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-14.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-14 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-14 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-14 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-14 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-14 .h2o-table th,\n",
       "#h2o-table-14 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5268031666585283</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>0</th>\n",
       "<th>1</th>\n",
       "<th>Error</th>\n",
       "<th>Rate</th></tr></thead>\n",
       "    <tbody><tr><td>0</td>\n",
       "<td>7544.0</td>\n",
       "<td>164.0</td>\n",
       "<td>0.0213</td>\n",
       "<td> (164.0/7708.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>544.0</td>\n",
       "<td>4607.0</td>\n",
       "<td>0.1056</td>\n",
       "<td> (544.0/5151.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>8088.0</td>\n",
       "<td>4771.0</td>\n",
       "<td>0.0551</td>\n",
       "<td> (708.0/12859.0)</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-15.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-15 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-15 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-15 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-15 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-15 .h2o-table th,\n",
       "#h2o-table-15 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-15 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-15\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
       "    <thead><tr><th>metric</th>\n",
       "<th>threshold</th>\n",
       "<th>value</th>\n",
       "<th>idx</th></tr></thead>\n",
       "    <tbody><tr><td>max f1</td>\n",
       "<td>0.5268032</td>\n",
       "<td>0.9286434</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1963658</td>\n",
       "<td>0.9320900</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.8195838</td>\n",
       "<td>0.9622204</td>\n",
       "<td>68.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5268032</td>\n",
       "<td>0.9449413</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998972</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0054836</td>\n",
       "<td>1.0</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998972</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5585317</td>\n",
       "<td>0.8858062</td>\n",
       "<td>127.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2576292</td>\n",
       "<td>0.9341875</td>\n",
       "<td>219.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3678220</td>\n",
       "<td>0.9382164</td>\n",
       "<td>182.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9998972</td>\n",
       "<td>7708.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9998972</td>\n",
       "<td>4009.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0010929</td>\n",
       "<td>7708.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0054836</td>\n",
       "<td>5151.0</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9998972</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9998972</td>\n",
       "<td>0.7782955</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0010929</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0054836</td>\n",
       "<td>1.0</td>\n",
       "<td>390.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-16.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-16 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-16 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-16 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-16 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-16 .h2o-table th,\n",
       "#h2o-table-16 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-16 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-16\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Gains/Lift Table: Avg response rate: 40.06 %, avg score: 40.06 %</caption>\n",
       "    <thead><tr><th>group</th>\n",
       "<th>cumulative_data_fraction</th>\n",
       "<th>lower_threshold</th>\n",
       "<th>lift</th>\n",
       "<th>cumulative_lift</th>\n",
       "<th>response_rate</th>\n",
       "<th>score</th>\n",
       "<th>cumulative_response_rate</th>\n",
       "<th>cumulative_score</th>\n",
       "<th>capture_rate</th>\n",
       "<th>cumulative_capture_rate</th>\n",
       "<th>gain</th>\n",
       "<th>cumulative_gain</th>\n",
       "<th>kolmogorov_smirnov</th></tr></thead>\n",
       "    <tbody><tr><td>1</td>\n",
       "<td>0.0100319</td>\n",
       "<td>0.9999773</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999893</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999893</td>\n",
       "<td>0.0250437</td>\n",
       "<td>0.0250437</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.0250437</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200638</td>\n",
       "<td>0.9999566</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999664</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999778</td>\n",
       "<td>0.0250437</td>\n",
       "<td>0.0500874</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.0500874</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300179</td>\n",
       "<td>0.9999370</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999464</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999674</td>\n",
       "<td>0.0248495</td>\n",
       "<td>0.0749369</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.0749369</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400498</td>\n",
       "<td>0.9999167</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999269</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999573</td>\n",
       "<td>0.0250437</td>\n",
       "<td>0.0999806</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.0999806</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500039</td>\n",
       "<td>0.9998937</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999062</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999471</td>\n",
       "<td>0.0248495</td>\n",
       "<td>0.1248301</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.1248301</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000078</td>\n",
       "<td>0.9997264</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998239</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998855</td>\n",
       "<td>0.1248301</td>\n",
       "<td>0.2496603</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.2496603</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500117</td>\n",
       "<td>0.9994316</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995915</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997875</td>\n",
       "<td>0.1248301</td>\n",
       "<td>0.3744904</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.3744904</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000156</td>\n",
       "<td>0.9989720</td>\n",
       "<td>2.4964085</td>\n",
       "<td>2.4964085</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9992197</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9996456</td>\n",
       "<td>0.1248301</td>\n",
       "<td>0.4993205</td>\n",
       "<td>149.6408464</td>\n",
       "<td>149.6408464</td>\n",
       "<td>0.4993205</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3000233</td>\n",
       "<td>0.9943744</td>\n",
       "<td>2.4944672</td>\n",
       "<td>2.4957614</td>\n",
       "<td>0.9992224</td>\n",
       "<td>0.9976141</td>\n",
       "<td>0.9997408</td>\n",
       "<td>0.9989684</td>\n",
       "<td>0.2494661</td>\n",
       "<td>0.7487866</td>\n",
       "<td>149.4467245</td>\n",
       "<td>149.5761391</td>\n",
       "<td>0.7486569</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4000311</td>\n",
       "<td>0.3147476</td>\n",
       "<td>1.7529213</td>\n",
       "<td>2.3100514</td>\n",
       "<td>0.7021773</td>\n",
       "<td>0.7135433</td>\n",
       "<td>0.9253499</td>\n",
       "<td>0.9276121</td>\n",
       "<td>0.1753058</td>\n",
       "<td>0.9240924</td>\n",
       "<td>75.2921340</td>\n",
       "<td>131.0051378</td>\n",
       "<td>0.8742740</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5000389</td>\n",
       "<td>0.0974595</td>\n",
       "<td>0.4600691</td>\n",
       "<td>1.9400549</td>\n",
       "<td>0.1842924</td>\n",
       "<td>0.1738352</td>\n",
       "<td>0.7771384</td>\n",
       "<td>0.7768567</td>\n",
       "<td>0.0460105</td>\n",
       "<td>0.9701029</td>\n",
       "<td>-53.9930944</td>\n",
       "<td>94.0054914</td>\n",
       "<td>0.7841922</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.5999689</td>\n",
       "<td>0.0409131</td>\n",
       "<td>0.1729030</td>\n",
       "<td>1.6457205</td>\n",
       "<td>0.0692607</td>\n",
       "<td>0.0636088</td>\n",
       "<td>0.6592353</td>\n",
       "<td>0.6580591</td>\n",
       "<td>0.0172782</td>\n",
       "<td>0.9873811</td>\n",
       "<td>-82.7097001</td>\n",
       "<td>64.5720473</td>\n",
       "<td>0.6463069</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.6999767</td>\n",
       "<td>0.0216727</td>\n",
       "<td>0.0718251</td>\n",
       "<td>1.4208533</td>\n",
       "<td>0.0287714</td>\n",
       "<td>0.0301954</td>\n",
       "<td>0.5691590</td>\n",
       "<td>0.5683543</td>\n",
       "<td>0.0071831</td>\n",
       "<td>0.9945642</td>\n",
       "<td>-92.8174873</td>\n",
       "<td>42.0853301</td>\n",
       "<td>0.4914505</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.7999844</td>\n",
       "<td>0.0114853</td>\n",
       "<td>0.0427068</td>\n",
       "<td>1.2485682</td>\n",
       "<td>0.0171073</td>\n",
       "<td>0.0160230</td>\n",
       "<td>0.5001458</td>\n",
       "<td>0.4993062</td>\n",
       "<td>0.0042710</td>\n",
       "<td>0.9988352</td>\n",
       "<td>-95.7293168</td>\n",
       "<td>24.8568246</td>\n",
       "<td>0.3317361</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999922</td>\n",
       "<td>0.0055041</td>\n",
       "<td>0.0097061</td>\n",
       "<td>1.1109050</td>\n",
       "<td>0.0038880</td>\n",
       "<td>0.0082841</td>\n",
       "<td>0.4450013</td>\n",
       "<td>0.4447435</td>\n",
       "<td>0.0009707</td>\n",
       "<td>0.9998059</td>\n",
       "<td>-99.0293902</td>\n",
       "<td>11.0905002</td>\n",
       "<td>0.1665158</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0002636</td>\n",
       "<td>0.0019412</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0007776</td>\n",
       "<td>0.0032667</td>\n",
       "<td>0.4005755</td>\n",
       "<td>0.4005924</td>\n",
       "<td>0.0001941</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.8058780</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-17.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-17 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-17 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-17 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-17 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-17 .h2o-table th,\n",
       "#h2o-table-17 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-17 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-17\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Cross-Validation Metrics Summary: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>mean</th>\n",
       "<th>sd</th>\n",
       "<th>cv_1_valid</th>\n",
       "<th>cv_2_valid</th>\n",
       "<th>cv_3_valid</th>\n",
       "<th>cv_4_valid</th>\n",
       "<th>cv_5_valid</th></tr></thead>\n",
       "    <tbody><tr><td>accuracy</td>\n",
       "<td>0.9461265</td>\n",
       "<td>0.0049752</td>\n",
       "<td>0.9458222</td>\n",
       "<td>0.9484696</td>\n",
       "<td>0.9496567</td>\n",
       "<td>0.9376211</td>\n",
       "<td>0.9490628</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>0.9830382</td>\n",
       "<td>0.0028554</td>\n",
       "<td>0.9833813</td>\n",
       "<td>0.9846837</td>\n",
       "<td>0.9860457</td>\n",
       "<td>0.9785076</td>\n",
       "<td>0.9825725</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.0538735</td>\n",
       "<td>0.0049752</td>\n",
       "<td>0.0541778</td>\n",
       "<td>0.0515304</td>\n",
       "<td>0.0503432</td>\n",
       "<td>0.0623789</td>\n",
       "<td>0.0509372</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>138.6</td>\n",
       "<td>13.903237</td>\n",
       "<td>142.0</td>\n",
       "<td>133.0</td>\n",
       "<td>132.0</td>\n",
       "<td>161.0</td>\n",
       "<td>125.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>0.9513826</td>\n",
       "<td>0.0070192</td>\n",
       "<td>0.9551159</td>\n",
       "<td>0.9526305</td>\n",
       "<td>0.9429658</td>\n",
       "<td>0.9458606</td>\n",
       "<td>0.9603406</td></tr>\n",
       "<tr><td>f1</td>\n",
       "<td>0.9301233</td>\n",
       "<td>0.0081784</td>\n",
       "<td>0.9267286</td>\n",
       "<td>0.9318298</td>\n",
       "<td>0.9376181</td>\n",
       "<td>0.9177312</td>\n",
       "<td>0.9367089</td></tr>\n",
       "<tr><td>f2</td>\n",
       "<td>0.9099336</td>\n",
       "<td>0.0155957</td>\n",
       "<td>0.8999799</td>\n",
       "<td>0.9119182</td>\n",
       "<td>0.9323309</td>\n",
       "<td>0.8912267</td>\n",
       "<td>0.9142123</td></tr>\n",
       "<tr><td>lift_top_group</td>\n",
       "<td>2.4970932</td>\n",
       "<td>0.0768793</td>\n",
       "<td>2.5771878</td>\n",
       "<td>2.552918</td>\n",
       "<td>2.4550562</td>\n",
       "<td>2.513145</td>\n",
       "<td>2.3871596</td></tr>\n",
       "<tr><td>logloss</td>\n",
       "<td>0.1422423</td>\n",
       "<td>0.0115336</td>\n",
       "<td>0.1425736</td>\n",
       "<td>0.1348241</td>\n",
       "<td>0.1312528</td>\n",
       "<td>0.1610991</td>\n",
       "<td>0.1414619</td></tr>\n",
       "<tr><td>max_per_class_error</td>\n",
       "<td>0.1029730</td>\n",
       "<td>0.0208176</td>\n",
       "<td>0.1170108</td>\n",
       "<td>0.1008902</td>\n",
       "<td>0.0711611</td>\n",
       "<td>0.1256086</td>\n",
       "<td>0.1001945</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>mean_per_class_error</td>\n",
       "<td>0.0621005</td>\n",
       "<td>0.0075437</td>\n",
       "<td>0.0656750</td>\n",
       "<td>0.0603177</td>\n",
       "<td>0.0535985</td>\n",
       "<td>0.0731003</td>\n",
       "<td>0.0578112</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.0415177</td>\n",
       "<td>0.0038111</td>\n",
       "<td>0.0416885</td>\n",
       "<td>0.0393813</td>\n",
       "<td>0.0382881</td>\n",
       "<td>0.0479610</td>\n",
       "<td>0.0402699</td></tr>\n",
       "<tr><td>null_deviance</td>\n",
       "<td>3463.6226</td>\n",
       "<td>75.84444</td>\n",
       "<td>3503.6106</td>\n",
       "<td>3457.3157</td>\n",
       "<td>3545.042</td>\n",
       "<td>3469.7798</td>\n",
       "<td>3342.364</td></tr>\n",
       "<tr><td>pr_auc</td>\n",
       "<td>0.9810316</td>\n",
       "<td>0.0032488</td>\n",
       "<td>0.9802018</td>\n",
       "<td>0.9821742</td>\n",
       "<td>0.984365</td>\n",
       "<td>0.9758569</td>\n",
       "<td>0.9825601</td></tr>\n",
       "<tr><td>precision</td>\n",
       "<td>0.9661947</td>\n",
       "<td>0.0120021</td>\n",
       "<td>0.9750271</td>\n",
       "<td>0.9670213</td>\n",
       "<td>0.9465649</td>\n",
       "<td>0.9655914</td>\n",
       "<td>0.9767687</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.826988</td>\n",
       "<td>0.0163561</td>\n",
       "<td>0.8244404</td>\n",
       "<td>0.8347221</td>\n",
       "<td>0.8413986</td>\n",
       "<td>0.7998099</td>\n",
       "<td>0.8345689</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>0.8970270</td>\n",
       "<td>0.0208176</td>\n",
       "<td>0.8829892</td>\n",
       "<td>0.8991098</td>\n",
       "<td>0.9288390</td>\n",
       "<td>0.8743914</td>\n",
       "<td>0.8998054</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>731.50214</td>\n",
       "<td>60.796585</td>\n",
       "<td>747.3708</td>\n",
       "<td>695.96185</td>\n",
       "<td>688.2894</td>\n",
       "<td>831.59344</td>\n",
       "<td>694.295</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.2035943</td>\n",
       "<td>0.0091575</td>\n",
       "<td>0.2041775</td>\n",
       "<td>0.1984471</td>\n",
       "<td>0.1956735</td>\n",
       "<td>0.2189999</td>\n",
       "<td>0.2006736</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>0.978772</td>\n",
       "<td>0.0087032</td>\n",
       "<td>0.9856609</td>\n",
       "<td>0.9802548</td>\n",
       "<td>0.963964</td>\n",
       "<td>0.9794080</td>\n",
       "<td>0.9845722</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "<pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 8 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_1_AutoML_4_20230502_130509\n",
       "\n",
       "\n",
       "Model Summary for Stacked Ensemble: \n",
       "key                                   value\n",
       "------------------------------------  ----------------\n",
       "Stacking strategy                     cross_validation\n",
       "Number of base models (used / total)  5/5\n",
       "# GBM base models (used / total)      1/1\n",
       "# XGBoost base models (used / total)  2/2\n",
       "# DRF base models (used / total)      1/1\n",
       "# GLM base models (used / total)      1/1\n",
       "Metalearner algorithm                 GLM\n",
       "Metalearner fold assignment scheme    Random\n",
       "Metalearner nfolds                    5\n",
       "Metalearner fold_column\n",
       "Custom metalearner hyperparameters    None\n",
       "\n",
       "ModelMetricsBinomialGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.018577308498790433\n",
       "RMSE: 0.13629860050195097\n",
       "LogLoss: 0.07196636886804604\n",
       "AUC: 0.9979130824487391\n",
       "AUCPR: 0.9972344208163098\n",
       "Gini: 0.9958261648974782\n",
       "Null degrees of freedom: 10042\n",
       "Residual degrees of freedom: 10037\n",
       "Null deviance: 13528.444734878416\n",
       "Residual deviance: 1445.5164850835727\n",
       "AIC: 1457.5164850835727\n",
       "\n",
       "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3731470151993044\n",
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ---------------\n",
       "0      5935  78    0.013    (78.0/6013.0)\n",
       "1      119   3911  0.0295   (119.0/4030.0)\n",
       "Total  6054  3989  0.0196   (197.0/10043.0)\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.373147     0.975433  187\n",
       "max f2                       0.215451     0.977545  238\n",
       "max f0point5                 0.483306     0.983523  158\n",
       "max accuracy                 0.373147     0.980384  187\n",
       "max precision                0.999922     1         0\n",
       "max recall                   0.0209459    1         363\n",
       "max specificity              0.999922     1         0\n",
       "max absolute_mcc             0.373147     0.959143  187\n",
       "max min_per_class_accuracy   0.295648     0.97705   211\n",
       "max mean_per_class_accuracy  0.373147     0.97875   187\n",
       "max tns                      0.999922     6013      0\n",
       "max fns                      0.999922     3032      0\n",
       "max fps                      0.00111289   6013      399\n",
       "max tps                      0.0209459    4030      363\n",
       "max tnr                      0.999922     1         0\n",
       "max fnr                      0.999922     0.752357  0\n",
       "max fpr                      0.00111289   1         399\n",
       "max tpr                      0.0209459    1         363\n",
       "\n",
       "Gains/Lift Table: Avg response rate: 40.13 %, avg score: 40.35 %\n",
       "group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100568                   0.999984           2.49206     2.49206            1                0.999993    1                           0.999993            0.025062        0.025062                   149.206   149.206            0.025062\n",
       "2        0.0200139                   0.99997            2.49206     2.49206            1                0.999978    1                           0.999986            0.0248139       0.0498759                  149.206   149.206            0.0498759\n",
       "3        0.0300707                   0.999958           2.49206     2.49206            1                0.999964    1                           0.999978            0.025062        0.074938                   149.206   149.206            0.074938\n",
       "4        0.0400279                   0.999944           2.49206     2.49206            1                0.999951    1                           0.999972            0.0248139       0.0997519                  149.206   149.206            0.0997519\n",
       "5        0.0500846                   0.999929           2.49206     2.49206            1                0.999937    1                           0.999965            0.025062        0.124814                   149.206   149.206            0.124814\n",
       "6        0.10007                     0.99982            2.49206     2.49206            1                0.99988     1                           0.999922            0.124566        0.24938                    149.206   149.206            0.24938\n",
       "7        0.150055                    0.999634           2.49206     2.49206            1                0.999733    1                           0.999859            0.124566        0.373945                   149.206   149.206            0.373945\n",
       "8        0.20004                     0.999296           2.49206     2.49206            1                0.999479    1                           0.999764            0.124566        0.498511                   149.206   149.206            0.498511\n",
       "9        0.30001                     0.996455           2.49206     2.49206            1                0.998395    1                           0.999308            0.249132        0.747643                   149.206   149.206            0.747643\n",
       "10       0.39998                     0.347313           2.25129     2.43188            0.903386         0.779895    0.975853                    0.944468            0.225062        0.972705                   125.129   143.188            0.956573\n",
       "11       0.50005                     0.0771716          0.255405    1.99633            0.102488         0.156045    0.801075                    0.786689            0.0255583       0.998263                   -74.4595  99.6327            0.832123\n",
       "12       0.60002                     0.0349229          0.0148928   1.6662             0.0059761        0.0521331   0.668603                    0.664304            0.00148883      0.999752                   -98.5107  66.6198            0.667638\n",
       "13       0.69999                     0.0180587          0.00248213  1.42859            0.000996016      0.0256215   0.573257                    0.57309             0.000248139     1                          -99.7518  42.8592            0.501081\n",
       "14       0.79996                     0.00974993         0           1.25006            0                0.0134106   0.501618                    0.503147            0               1                          -100      25.0062            0.334109\n",
       "15       0.89993                     0.0048137          0           1.1112             0                0.00711417  0.445895                    0.448045            0               1                          -100      11.1197            0.167138\n",
       "16       1                           0.000637282        0           1                  0                0.00290986  0.401275                    0.4035              0               1                          -100      0                  0\n",
       "\n",
       "ModelMetricsBinomialGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 0.041538956172924135\n",
       "RMSE: 0.2038110796127731\n",
       "LogLoss: 0.14223209418142302\n",
       "AUC: 0.9830456110265015\n",
       "AUCPR: 0.9810762557763709\n",
       "Gini: 0.9660912220530029\n",
       "Null degrees of freedom: 12858\n",
       "Residual degrees of freedom: 12853\n",
       "Null deviance: 17318.11230397557\n",
       "Residual deviance: 3657.924998157837\n",
       "AIC: 3669.924998157837\n",
       "\n",
       "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5268031666585283\n",
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ---------------\n",
       "0      7544  164   0.0213   (164.0/7708.0)\n",
       "1      544   4607  0.1056   (544.0/5151.0)\n",
       "Total  8088  4771  0.0551   (708.0/12859.0)\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.526803     0.928643  135\n",
       "max f2                       0.196366     0.93209   244\n",
       "max f0point5                 0.819584     0.96222   68\n",
       "max accuracy                 0.526803     0.944941  135\n",
       "max precision                0.999897     1         0\n",
       "max recall                   0.00548356   1         390\n",
       "max specificity              0.999897     1         0\n",
       "max absolute_mcc             0.558532     0.885806  127\n",
       "max min_per_class_accuracy   0.257629     0.934188  219\n",
       "max mean_per_class_accuracy  0.367822     0.938216  182\n",
       "max tns                      0.999897     7708      0\n",
       "max fns                      0.999897     4009      0\n",
       "max fps                      0.00109288   7708      399\n",
       "max tps                      0.00548356   5151      390\n",
       "max tnr                      0.999897     1         0\n",
       "max fnr                      0.999897     0.778295  0\n",
       "max fpr                      0.00109288   1         399\n",
       "max tpr                      0.00548356   1         390\n",
       "\n",
       "Gains/Lift Table: Avg response rate: 40.06 %, avg score: 40.06 %\n",
       "group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100319                   0.999977           2.49641     2.49641            1                0.999989    1                           0.999989            0.0250437       0.0250437                  149.641   149.641            0.0250437\n",
       "2        0.0200638                   0.999957           2.49641     2.49641            1                0.999966    1                           0.999978            0.0250437       0.0500874                  149.641   149.641            0.0500874\n",
       "3        0.0300179                   0.999937           2.49641     2.49641            1                0.999946    1                           0.999967            0.0248495       0.0749369                  149.641   149.641            0.0749369\n",
       "4        0.0400498                   0.999917           2.49641     2.49641            1                0.999927    1                           0.999957            0.0250437       0.0999806                  149.641   149.641            0.0999806\n",
       "5        0.0500039                   0.999894           2.49641     2.49641            1                0.999906    1                           0.999947            0.0248495       0.12483                    149.641   149.641            0.12483\n",
       "6        0.100008                    0.999726           2.49641     2.49641            1                0.999824    1                           0.999885            0.12483         0.24966                    149.641   149.641            0.24966\n",
       "7        0.150012                    0.999432           2.49641     2.49641            1                0.999592    1                           0.999788            0.12483         0.37449                    149.641   149.641            0.37449\n",
       "8        0.200016                    0.998972           2.49641     2.49641            1                0.99922     1                           0.999646            0.12483         0.499321                   149.641   149.641            0.499321\n",
       "9        0.300023                    0.994374           2.49447     2.49576            0.999222         0.997614    0.999741                    0.998968            0.249466        0.748787                   149.447   149.576            0.748657\n",
       "10       0.400031                    0.314748           1.75292     2.31005            0.702177         0.713543    0.92535                     0.927612            0.175306        0.924092                   75.2921   131.005            0.874274\n",
       "11       0.500039                    0.0974595          0.460069    1.94005            0.184292         0.173835    0.777138                    0.776857            0.0460105       0.970103                   -53.9931  94.0055            0.784192\n",
       "12       0.599969                    0.0409131          0.172903    1.64572            0.0692607        0.0636088   0.659235                    0.658059            0.0172782       0.987381                   -82.7097  64.572             0.646307\n",
       "13       0.699977                    0.0216727          0.0718251   1.42085            0.0287714        0.0301954   0.569159                    0.568354            0.00718307      0.994564                   -92.8175  42.0853            0.491451\n",
       "14       0.799984                    0.0114853          0.0427068   1.24857            0.0171073        0.016023    0.500146                    0.499306            0.00427102      0.998835                   -95.7293  24.8568            0.331736\n",
       "15       0.899992                    0.00550413         0.0097061   1.11091            0.00388802       0.00828414  0.445001                    0.444743            0.000970685     0.999806                   -99.0294  11.0905            0.166516\n",
       "16       1                           0.00026361         0.00194122  1                  0.000777605      0.00326668  0.400575                    0.400592            0.000194137     1                          -99.8059  0                  0\n",
       "\n",
       "Cross-Validation Metrics Summary: \n",
       "                      mean         sd            cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "--------------------  -----------  ------------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy              0.94612646   0.0049751666  0.9458222     0.9484696     0.9496567     0.93762106    0.94906276\n",
       "auc                   0.9830382    0.0028554476  0.98338133    0.98468375    0.9860457     0.9785076     0.9825725\n",
       "err                   0.053873524  0.0049751666  0.054177795   0.051530413   0.05034325    0.062378924   0.050937247\n",
       "err_count             138.6        13.903237     142.0         133.0         132.0         161.0         125.0\n",
       "f0point5              0.95138264   0.007019179   0.9551159     0.95263046    0.9429658     0.94586056    0.96034056\n",
       "f1                    0.9301233    0.00817836    0.9267286     0.9318298     0.93761814    0.9177312     0.93670887\n",
       "f2                    0.90993357   0.0155957155  0.89997995    0.91191816    0.93233085    0.89122665    0.9142123\n",
       "lift_top_group        2.4970932    0.07687934    2.5771878     2.552918      2.4550562     2.513145      2.3871596\n",
       "logloss               0.14224228   0.011533612   0.1425736     0.13482408    0.13125275    0.16109908    0.1414619\n",
       "max_per_class_error   0.10297304   0.020817585   0.11701082    0.100890204   0.07116105    0.12560856    0.10019455\n",
       "---                   ---          ---           ---           ---           ---           ---           ---\n",
       "mean_per_class_error  0.06210054   0.0075437464  0.06567498    0.060317714   0.05359854    0.07310029    0.05781116\n",
       "mse                   0.041517742  0.003811057   0.041688457   0.03938126    0.038288113   0.04796098    0.040269908\n",
       "null_deviance         3463.6226    75.84444      3503.6106     3457.3157     3545.042      3469.7798     3342.364\n",
       "pr_auc                0.9810316    0.0032488222  0.98020184    0.98217416    0.984365      0.9758569     0.9825601\n",
       "precision             0.9661947    0.012002076   0.97502714    0.9670213     0.9465649     0.9655914     0.97676873\n",
       "r2                    0.826988     0.016356122   0.8244404     0.8347221     0.8413986     0.7998099     0.8345689\n",
       "recall                0.89702696   0.020817585   0.88298917    0.8991098     0.92883897    0.87439144    0.8998054\n",
       "residual_deviance     731.50214    60.796585     747.3708      695.96185     688.2894      831.59344     694.295\n",
       "rmse                  0.20359434   0.009157475   0.20417751    0.19844712    0.19567348    0.21899995    0.20067364\n",
       "specificity           0.978772     0.008703159   0.98566085    0.98025477    0.963964      0.97940797    0.98457223\n",
       "[22 rows x 8 columns]\n",
       "\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = H2OAutoML(max_models=5, seed=42)\n",
    "clf.train(x=features, y=target, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5199d98f-7d73-4f8a-8716-4fd6d8dae44c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.03905347189587536\n",
      "RMSE: 0.19761951294311844\n",
      "LogLoss: 0.13756448955386694\n",
      "AUC: 0.9826964433349686\n",
      "AUCPR: 0.9811362492301363\n",
      "Gini: 0.9653928866699373\n",
      "Null degrees of freedom: 3225\n",
      "Residual degrees of freedom: 3220\n",
      "Null deviance: 4336.310320957466\n",
      "Residual deviance: 887.5660866015493\n",
      "AIC: 899.5660866015493\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4938344501996043\n",
      "       0     1     Error    Rate\n",
      "-----  ----  ----  -------  --------------\n",
      "0      1902  41    0.0211   (41.0/1943.0)\n",
      "1      115   1168  0.0896   (115.0/1283.0)\n",
      "Total  2017  1209  0.0484   (156.0/3226.0)\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "metric                       threshold    value     idx\n",
      "---------------------------  -----------  --------  -----\n",
      "max f1                       0.493834     0.9374    137\n",
      "max f2                       0.269826     0.930863  196\n",
      "max f0point5                 0.674595     0.967935  99\n",
      "max accuracy                 0.493834     0.951643  137\n",
      "max precision                0.999924     1         0\n",
      "max recall                   0.0108424    1         378\n",
      "max specificity              0.999924     1         0\n",
      "max absolute_mcc             0.658248     0.900053  103\n",
      "max min_per_class_accuracy   0.269826     0.936087  196\n",
      "max mean_per_class_accuracy  0.493834     0.944632  137\n",
      "max tns                      0.999924     1943      0\n",
      "max fns                      0.999924     1064      0\n",
      "max fps                      0.000957771  1943      399\n",
      "max tps                      0.0108424    1283      378\n",
      "max tnr                      0.999924     1         0\n",
      "max fnr                      0.999924     0.829306  0\n",
      "max fpr                      0.000957771  1         399\n",
      "max tpr                      0.0108424    1         378\n",
      "\n",
      "Gains/Lift Table: Avg response rate: 39.77 %, avg score: 40.07 %\n",
      "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
      "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
      "1        0.0102294                   0.99997            2.51442    2.51442            1                0.99998     1                           0.99998             0.025721        0.025721                   151.442   151.442            0.025721\n",
      "2        0.0201488                   0.999952           2.51442    2.51442            1                0.99996     1                           0.99997             0.0249415       0.0506625                  151.442   151.442            0.0506625\n",
      "3        0.0300682                   0.999935           2.51442    2.51442            1                0.999943    1                           0.999961            0.0249415       0.0756041                  151.442   151.442            0.0756041\n",
      "4        0.0402976                   0.999914           2.51442    2.51442            1                0.999924    1                           0.999952            0.025721        0.101325                   151.442   151.442            0.101325\n",
      "5        0.050217                    0.9999             2.51442    2.51442            1                0.999907    1                           0.999943            0.0249415       0.126267                   151.442   151.442            0.126267\n",
      "6        0.100124                    0.999767           2.51442    2.51442            1                0.999837    1                           0.99989             0.125487        0.251754                   151.442   151.442            0.251754\n",
      "7        0.150031                    0.999509           2.51442    2.51442            1                0.999655    1                           0.999812            0.125487        0.377241                   151.442   151.442            0.377241\n",
      "8        0.200248                    0.999092           2.51442    2.51442            1                0.999317    1                           0.999688            0.126267        0.503507                   151.442   151.442            0.503507\n",
      "9        0.300062                    0.994742           2.51442    2.51442            1                0.997851    1                           0.999077            0.250974        0.754482                   151.442   151.442            0.754482\n",
      "10       0.400186                    0.313013           1.73596    2.31965            0.690402         0.708243    0.922541                    0.926312            0.173811        0.928293                   73.5961   131.965            0.876826\n",
      "11       0.5                         0.100257           0.382629   1.93297            0.152174         0.179216    0.768754                    0.777171            0.0381917       0.966485                   -61.7371  93.297             0.774514\n",
      "12       0.600124                    0.0429305          0.171261   1.63905            0.0681115        0.0660844   0.65186                     0.658534            0.0171473       0.983632                   -82.8739  63.9048            0.636746\n",
      "13       0.699938                    0.0199234          0.0937051  1.41868            0.0372671        0.0295355   0.564216                    0.568836            0.00935308      0.992985                   -90.6295  41.8676            0.486552\n",
      "14       0.800062                    0.0103871          0.0700612  1.2499             0.0278638        0.0147229   0.497094                    0.499492            0.00701481      1                          -92.9939  24.9903            0.331961\n",
      "15       0.899876                    0.0049464          0          1.11126            0                0.00760602  0.441957                    0.444932            0               1                          -100      11.1264            0.166238\n",
      "16       1                           0.000703194        0          1                  0                0.00295535  0.397706                    0.400679            0               1                          -100      0                  0\n"
     ]
    }
   ],
   "source": [
    "perf = clf.leader.model_performance(test_data=test)\n",
    "print(perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaeb04-6fe4-4db0-b6df-94390029325e",
   "metadata": {},
   "source": [
    "# DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15dd574c-58ce-475b-9be9-0d8c69e497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstLayer = 25\n",
    "SecondLayer = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "482cda06-92bc-4e9d-bf57-5c551bfb2240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: |█████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "def calculate_f1_score(metrics):\n",
    "    actual = valid_data_h2o[response_column].as_data_frame()\n",
    "    predicted = model.predict(valid_data_h2o).as_data_frame()\n",
    "    f1 = f1_score(actual, predicted)\n",
    "    metrics[\"f1_score\"] = f1\n",
    "checkpoint_dir = \"/checkpoint\"\n",
    "\n",
    "dl = H2ODeepLearningEstimator(\n",
    "                           distribution = 'bernoulli',\n",
    "                           hidden=[FirstLayer,SecondLayer],\n",
    "                           epochs=10,\n",
    "                           train_samples_per_iteration=-1,\n",
    "                           reproducible=True,\n",
    "                           activation=\"tanh\",\n",
    "                            use_all_factor_levels=True,\n",
    "                           single_node_mode=False,\n",
    "                           balance_classes=False,\n",
    "                           force_load_balance=False,\n",
    "                           seed=42,\n",
    "                           score_training_samples=0,\n",
    "                           score_validation_samples=0,\n",
    "                           stopping_rounds=0,\n",
    "    score_each_iteration=True,\n",
    "    loss='cross_entropy',\n",
    "    standardize=True,\n",
    "\n",
    ")\n",
    "\n",
    "dl.train(x=features,\n",
    "      y=target,\n",
    "      training_frame=data\n",
    "        )\n",
    "\n",
    "weights = dl._model_json['output']\n",
    "# print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "753c7ef1-55bf-4e4c-93c3-234d65c1349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_history = dl.scoring_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3975a9d2-f1ce-40f9-a796-b543a2ce5fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>training_speed</th>\n",
       "      <th>epochs</th>\n",
       "      <th>iterations</th>\n",
       "      <th>samples</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_r2</th>\n",
       "      <th>training_auc</th>\n",
       "      <th>training_pr_auc</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:37:55</td>\n",
       "      <td>0.000 sec</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:01</td>\n",
       "      <td>6.210 sec</td>\n",
       "      <td>3099 obs/sec</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16085.0</td>\n",
       "      <td>0.369765</td>\n",
       "      <td>0.424033</td>\n",
       "      <td>0.430308</td>\n",
       "      <td>0.879816</td>\n",
       "      <td>0.833526</td>\n",
       "      <td>2.484472</td>\n",
       "      <td>0.202549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:06</td>\n",
       "      <td>12.023 sec</td>\n",
       "      <td>3148 obs/sec</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>32170.0</td>\n",
       "      <td>0.356927</td>\n",
       "      <td>0.399082</td>\n",
       "      <td>0.469179</td>\n",
       "      <td>0.895748</td>\n",
       "      <td>0.858336</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.190923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:12</td>\n",
       "      <td>17.940 sec</td>\n",
       "      <td>3147 obs/sec</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>48255.0</td>\n",
       "      <td>0.344329</td>\n",
       "      <td>0.375920</td>\n",
       "      <td>0.505989</td>\n",
       "      <td>0.908127</td>\n",
       "      <td>0.872808</td>\n",
       "      <td>2.468944</td>\n",
       "      <td>0.169537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:18</td>\n",
       "      <td>23.920 sec</td>\n",
       "      <td>3127 obs/sec</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>64340.0</td>\n",
       "      <td>0.339997</td>\n",
       "      <td>0.367609</td>\n",
       "      <td>0.518342</td>\n",
       "      <td>0.914076</td>\n",
       "      <td>0.881593</td>\n",
       "      <td>2.468944</td>\n",
       "      <td>0.162387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:24</td>\n",
       "      <td>29.966 sec</td>\n",
       "      <td>3107 obs/sec</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>80425.0</td>\n",
       "      <td>0.329089</td>\n",
       "      <td>0.347883</td>\n",
       "      <td>0.548750</td>\n",
       "      <td>0.922406</td>\n",
       "      <td>0.891521</td>\n",
       "      <td>2.468944</td>\n",
       "      <td>0.152502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:30</td>\n",
       "      <td>35.674 sec</td>\n",
       "      <td>3128 obs/sec</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>96510.0</td>\n",
       "      <td>0.322236</td>\n",
       "      <td>0.334634</td>\n",
       "      <td>0.567350</td>\n",
       "      <td>0.928577</td>\n",
       "      <td>0.900027</td>\n",
       "      <td>2.484472</td>\n",
       "      <td>0.143612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:36</td>\n",
       "      <td>41.392 sec</td>\n",
       "      <td>3144 obs/sec</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>112595.0</td>\n",
       "      <td>0.313009</td>\n",
       "      <td>0.321419</td>\n",
       "      <td>0.591772</td>\n",
       "      <td>0.936127</td>\n",
       "      <td>0.910675</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.132732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:42</td>\n",
       "      <td>47.188 sec</td>\n",
       "      <td>3149 obs/sec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>128680.0</td>\n",
       "      <td>0.308428</td>\n",
       "      <td>0.310110</td>\n",
       "      <td>0.603634</td>\n",
       "      <td>0.938395</td>\n",
       "      <td>0.912585</td>\n",
       "      <td>2.484472</td>\n",
       "      <td>0.130619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:47</td>\n",
       "      <td>53.407 sec</td>\n",
       "      <td>3147 obs/sec</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>144765.0</td>\n",
       "      <td>0.299653</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.625867</td>\n",
       "      <td>0.945197</td>\n",
       "      <td>0.921591</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.120609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2023-07-03 13:38:54</td>\n",
       "      <td>59.229 sec</td>\n",
       "      <td>3149 obs/sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>160850.0</td>\n",
       "      <td>0.287512</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>0.655570</td>\n",
       "      <td>0.952173</td>\n",
       "      <td>0.929669</td>\n",
       "      <td>2.484472</td>\n",
       "      <td>0.111284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration training_speed  epochs  iterations  \\\n",
       "0     2023-07-03 13:37:55   0.000 sec           None     0.0           0   \n",
       "1     2023-07-03 13:38:01   6.210 sec   3099 obs/sec     1.0           1   \n",
       "2     2023-07-03 13:38:06  12.023 sec   3148 obs/sec     2.0           2   \n",
       "3     2023-07-03 13:38:12  17.940 sec   3147 obs/sec     3.0           3   \n",
       "4     2023-07-03 13:38:18  23.920 sec   3127 obs/sec     4.0           4   \n",
       "5     2023-07-03 13:38:24  29.966 sec   3107 obs/sec     5.0           5   \n",
       "6     2023-07-03 13:38:30  35.674 sec   3128 obs/sec     6.0           6   \n",
       "7     2023-07-03 13:38:36  41.392 sec   3144 obs/sec     7.0           7   \n",
       "8     2023-07-03 13:38:42  47.188 sec   3149 obs/sec     8.0           8   \n",
       "9     2023-07-03 13:38:47  53.407 sec   3147 obs/sec     9.0           9   \n",
       "10    2023-07-03 13:38:54  59.229 sec   3149 obs/sec    10.0          10   \n",
       "\n",
       "     samples  training_rmse  training_logloss  training_r2  training_auc  \\\n",
       "0        0.0            NaN               NaN          NaN           NaN   \n",
       "1    16085.0       0.369765          0.424033     0.430308      0.879816   \n",
       "2    32170.0       0.356927          0.399082     0.469179      0.895748   \n",
       "3    48255.0       0.344329          0.375920     0.505989      0.908127   \n",
       "4    64340.0       0.339997          0.367609     0.518342      0.914076   \n",
       "5    80425.0       0.329089          0.347883     0.548750      0.922406   \n",
       "6    96510.0       0.322236          0.334634     0.567350      0.928577   \n",
       "7   112595.0       0.313009          0.321419     0.591772      0.936127   \n",
       "8   128680.0       0.308428          0.310110     0.603634      0.938395   \n",
       "9   144765.0       0.299653          0.296592     0.625867      0.945197   \n",
       "10  160850.0       0.287512          0.276586     0.655570      0.952173   \n",
       "\n",
       "    training_pr_auc  training_lift  training_classification_error  \n",
       "0               NaN            NaN                            NaN  \n",
       "1          0.833526       2.484472                       0.202549  \n",
       "2          0.858336       2.500000                       0.190923  \n",
       "3          0.872808       2.468944                       0.169537  \n",
       "4          0.881593       2.468944                       0.162387  \n",
       "5          0.891521       2.468944                       0.152502  \n",
       "6          0.900027       2.484472                       0.143612  \n",
       "7          0.910675       2.500000                       0.132732  \n",
       "8          0.912585       2.484472                       0.130619  \n",
       "9          0.921591       2.500000                       0.120609  \n",
       "10         0.929669       2.484472                       0.111284  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69e8d5-bec3-4908-83b7-9e9406768d87",
   "metadata": {},
   "source": [
    "### Creating TensorFlow model Based on H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb25f469-555c-4333-920a-575e42877d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 608)\n",
      "(50, 25)\n",
      "(2, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_keys = dl._model_json['output']['weights']\n",
    "\n",
    "# get the weight values as numpy arrays\n",
    "weights = []\n",
    "for key in weight_keys:\n",
    "    weight_frame = h2o.get_frame(key['name'])\n",
    "    weight_df = weight_frame.as_data_frame()\n",
    "    weight_values = weight_df.values\n",
    "    print(weight_values.shape)\n",
    "    weights.append(weight_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79b03ca-5bf9-49bb-9289-c0fab7a8579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n",
      "(50, 1)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "norm = dl._model_json['output']['normmul']\n",
    "\n",
    "biases_keys = dl._model_json['output']['biases']\n",
    "\n",
    "# get the weight values as numpy arrays\n",
    "biases = []\n",
    "for key in biases_keys:\n",
    "    biases_frame = h2o.get_frame(key['name'])\n",
    "    biases_df = biases_frame.as_data_frame()\n",
    "    biases_values = biases_df.values\n",
    "    print(biases_values.shape)\n",
    "    biases.append(biases_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaaa6f9a-739b-49bb-8304-049ce5046cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(608,)),\n",
    "    tf.keras.layers.Dense(units=FirstLayer, activation='tanh',\n",
    "                          ),\n",
    "    tf.keras.layers.Dense(units=SecondLayer, activation='tanh',\n",
    "                          ),\n",
    "    tf.keras.layers.Dense(units=1, activation='linear',\n",
    "                          )\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c98f2f7-ceba-464d-922c-c17045810d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(608,)),\n",
    "    tf.keras.layers.Dense(units=FirstLayer, activation='tanh',\n",
    "                          ),\n",
    "    tf.keras.layers.Dense(units=SecondLayer, activation='tanh',\n",
    "                          ),\n",
    "    tf.keras.layers.Dense(units=2, activation='softmax',\n",
    "                          )\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52184c4a-315b-4baa-9a56-2dab25a13f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(len(weights)):\n",
    "    l = np.reshape(biases[i], (biases[i].shape[0],))\n",
    "    model.layers[i].set_weights([weights[i].T, l])\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b92e40e-afd5-420f-b12c-ac1d4501888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTensor = train.as_data_frame()\n",
    "validTensor = valid.as_data_frame()\n",
    "testTensor  = test.as_data_frame()\n",
    "trainTensorY = pd.get_dummies(pd.get_dummies(trainTensor[target]))\n",
    "trainTensor = trainTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "validTensorY = pd.get_dummies(pd.get_dummies(validTensor[target]))\n",
    "validTensor = validTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "testTensorY = pd.get_dummies(pd.get_dummies(testTensor[target]))\n",
    "testTensor = testTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702a842-e42a-4dd7-ac11-fcc7861b2ece",
   "metadata": {},
   "source": [
    "## All In ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27b1fc5e-1f1a-4512-b25c-c4f1b09ca13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: |█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4424 - accuracy: 0.8045 - val_loss: 0.3932 - val_accuracy: 0.8300 - 836ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "██████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4668 - accuracy: 0.7987 - val_loss: 0.4012 - val_accuracy: 0.8288 - 959ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4973 - accuracy: 0.7925 - val_loss: 0.3971 - val_accuracy: 0.8405 - 804ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4715 - accuracy: 0.7927 - val_loss: 0.3817 - val_accuracy: 0.8282 - 777ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5608 - accuracy: 0.7600 - val_loss: 0.4826 - val_accuracy: 0.7960 - 801ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6017 - accuracy: 0.7570 - val_loss: 0.5092 - val_accuracy: 0.7886 - 809ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4726 - accuracy: 0.7859 - val_loss: 0.4216 - val_accuracy: 0.8183 - 897ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5198 - accuracy: 0.7750 - val_loss: 0.4425 - val_accuracy: 0.8164 - 803ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5676 - accuracy: 0.7680 - val_loss: 0.5441 - val_accuracy: 0.7608 - 858ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4190 - accuracy: 0.8172 - val_loss: 0.3491 - val_accuracy: 0.8578 - 794ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4716 - accuracy: 0.7998 - val_loss: 0.3689 - val_accuracy: 0.8548 - 833ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5433 - accuracy: 0.7930 - val_loss: 0.4543 - val_accuracy: 0.8183 - 905ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3973 - accuracy: 0.8257 - val_loss: 0.3082 - val_accuracy: 0.8733 - 931ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4769 - accuracy: 0.8209 - val_loss: 0.2769 - val_accuracy: 0.8832 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6889 - accuracy: 0.8042 - val_loss: 0.4042 - val_accuracy: 0.8603 - 957ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3840 - accuracy: 0.8395 - val_loss: 0.3133 - val_accuracy: 0.8702 - 954ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5012 - accuracy: 0.8291 - val_loss: 0.2780 - val_accuracy: 0.8925 - 875ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.7697 - accuracy: 0.8030 - val_loss: 0.5266 - val_accuracy: 0.8511 - 895ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4536 - accuracy: 0.8105 - val_loss: 0.3037 - val_accuracy: 0.8727 - 840ms/epoch - 2ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6046 - accuracy: 0.8052 - val_loss: 0.3142 - val_accuracy: 0.8764 - 899ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.9439 - accuracy: 0.7783 - val_loss: 0.5284 - val_accuracy: 0.8684 - 922ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4435 - accuracy: 0.8121 - val_loss: 0.3150 - val_accuracy: 0.8764 - 968ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4416 - accuracy: 0.8453 - val_loss: 0.2126 - val_accuracy: 0.9209 - 979ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6587 - accuracy: 0.8330 - val_loss: 0.1881 - val_accuracy: 0.9425 - 1s/epoch - 4ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3850 - accuracy: 0.8414 - val_loss: 0.2464 - val_accuracy: 0.8993 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4652 - accuracy: 0.8594 - val_loss: 0.1616 - val_accuracy: 0.9413 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 1.0603 - accuracy: 0.7879 - val_loss: 0.5756 - val_accuracy: 0.8603 - 988ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3112 - accuracy: 0.8762 - val_loss: 0.1857 - val_accuracy: 0.9240 - 932ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3809 - accuracy: 0.8780 - val_loss: 0.1509 - val_accuracy: 0.9431 - 995ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6233 - accuracy: 0.8601 - val_loss: 0.2474 - val_accuracy: 0.9388 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3484 - accuracy: 0.8665 - val_loss: 0.1778 - val_accuracy: 0.9283 - 1s/epoch - 4ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5253 - accuracy: 0.8513 - val_loss: 0.1851 - val_accuracy: 0.9302 - 977ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.8441 - accuracy: 0.8286 - val_loss: 0.3995 - val_accuracy: 0.9036 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3577 - accuracy: 0.8640 - val_loss: 0.1694 - val_accuracy: 0.9333 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5226 - accuracy: 0.8620 - val_loss: 0.1513 - val_accuracy: 0.9475 - 893ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.7531 - accuracy: 0.8581 - val_loss: 0.2198 - val_accuracy: 0.9444 - 992ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3917 - accuracy: 0.8513 - val_loss: 0.3324 - val_accuracy: 0.8739 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4907 - accuracy: 0.8564 - val_loss: 0.2618 - val_accuracy: 0.9067 - 1s/epoch - 4ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.7707 - accuracy: 0.8375 - val_loss: 0.5449 - val_accuracy: 0.8739 - 1s/epoch - 4ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4261 - accuracy: 0.8554 - val_loss: 0.2910 - val_accuracy: 0.8900 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.6351 - accuracy: 0.8341 - val_loss: 0.4049 - val_accuracy: 0.8795 - 948ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.9022 - accuracy: 0.8358 - val_loss: 0.5132 - val_accuracy: 0.8857 - 947ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3042 - accuracy: 0.8914 - val_loss: 0.1151 - val_accuracy: 0.9555 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4257 - accuracy: 0.8897 - val_loss: 0.1147 - val_accuracy: 0.9580 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3757 - accuracy: 0.9157 - val_loss: 0.1190 - val_accuracy: 0.9679 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.3354 - accuracy: 0.8818 - val_loss: 0.1632 - val_accuracy: 0.9382 - 910ms/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.4330 - accuracy: 0.8884 - val_loss: 0.1527 - val_accuracy: 0.9518 - 1s/epoch - 3ms/step\n",
      "deeplearning Model Build progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid-18/CHS/jakeluo/envs/Amirsajjad/lib/python3.10/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: Dropping bad and constant columns: [RSP004, EAR001, RHB003, PNS003, GIS014, END017, INF011, URN012, URN007, NEO048, MHT003, PRG028, ENT015, MHT002, NEO051]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█████████████████████████████████████████████| (done) 100%\n",
      "354/354 - 1s - loss: 0.5201 - accuracy: 0.8917 - val_loss: 0.2491 - val_accuracy: 0.9382 - 985ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "FirstLayer = 25\n",
    "SecondLayer = 50\n",
    "epoch = 10\n",
    "for FirstLayer in [10,25,50,100]:\n",
    "    for SecondLayer in [10,25,50,100]:\n",
    "        for epoch in [25,50,100]:\n",
    "            dl = H2ODeepLearningEstimator(\n",
    "                                       distribution = 'bernoulli',\n",
    "                                       hidden=[FirstLayer,SecondLayer],\n",
    "                                       epochs=epoch,\n",
    "                                       train_samples_per_iteration=-1,\n",
    "                                       reproducible=True,\n",
    "                                       activation=\"tanh\",\n",
    "                                       single_node_mode=False,\n",
    "                                       balance_classes=False,\n",
    "                                       force_load_balance=False,\n",
    "                                       seed=42,\n",
    "                                       score_training_samples=0,\n",
    "                                       score_validation_samples=0,\n",
    "                                        nfolds=5,\n",
    "                                       stopping_rounds=0,\n",
    "                loss='cross_entropy',\n",
    "                standardize=True,\n",
    "                export_weights_and_biases = True)\n",
    "\n",
    "            dl.train(x=features,\n",
    "                      y=target,\n",
    "                      training_frame=data\n",
    "                     ,\n",
    "                    validation_frame=test)\n",
    "\n",
    "            weights = dl._model_json['output']\n",
    "            # print(weights)\n",
    "\n",
    "            weight_keys = dl._model_json['output']['weights']\n",
    "\n",
    "            # get the weight values as numpy arrays\n",
    "            weights = []\n",
    "            for key in weight_keys:\n",
    "                weight_frame = h2o.get_frame(key['name'])\n",
    "                weight_df = weight_frame.as_data_frame()\n",
    "                weight_values = weight_df.values\n",
    "                # print(weight_values.shape)\n",
    "                weights.append(weight_values)\n",
    "\n",
    "\n",
    "            norm = dl._model_json['output']['normmul']\n",
    "\n",
    "            biases_keys = dl._model_json['output']['biases']\n",
    "\n",
    "            # get the weight values as numpy arrays\n",
    "            biases = []\n",
    "            for key in biases_keys:\n",
    "                biases_frame = h2o.get_frame(key['name'])\n",
    "                biases_df = biases_frame.as_data_frame()\n",
    "                biases_values = biases_df.values\n",
    "                # print(biases_values.shape)\n",
    "                biases.append(biases_values)\n",
    "\n",
    "            import tensorflow as tf\n",
    "\n",
    "            # Define the model architecture\n",
    "            model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.InputLayer(input_shape=(608,)),\n",
    "                tf.keras.layers.Dense(units=FirstLayer, activation='tanh',\n",
    "                                      ),\n",
    "                tf.keras.layers.Dense(units=SecondLayer, activation='tanh',\n",
    "                                      ),\n",
    "                tf.keras.layers.Dense(units=1, activation='linear',\n",
    "                                      )\n",
    "            ])\n",
    "\n",
    "            # Compile the model\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "            import tensorflow as tf\n",
    "\n",
    "            # Define the model architecture\n",
    "            model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.InputLayer(input_shape=(608,)),\n",
    "                tf.keras.layers.Dense(units=FirstLayer, activation='tanh',\n",
    "                                      ),\n",
    "                tf.keras.layers.Dense(units=SecondLayer, activation='tanh',\n",
    "                                      ),\n",
    "                tf.keras.layers.Dense(units=2, activation='softmax',\n",
    "                                      )\n",
    "            ])\n",
    "\n",
    "            # Compile the model\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "            import numpy as np\n",
    "            for i in range(len(weights)):\n",
    "                l = np.reshape(biases[i], (biases[i].shape[0],))\n",
    "                model.layers[i].set_weights([weights[i].T, l])\n",
    "                # print(i)\n",
    "\n",
    "            trainTensor = train.as_data_frame()\n",
    "            validTensor = valid.as_data_frame()\n",
    "            testTensor  = test.as_data_frame()\n",
    "            trainTensorY = pd.get_dummies(pd.get_dummies(trainTensor[target]))\n",
    "            trainTensor = trainTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "            validTensorY = pd.get_dummies(pd.get_dummies(validTensor[target]))\n",
    "            validTensor = validTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "            testTensorY = pd.get_dummies(pd.get_dummies(testTensor[target]))\n",
    "            testTensor = testTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "\n",
    "            model.fit(trainTensor, trainTensorY, epochs=1,verbose=2, validation_data=(validTensor, validTensorY))\n",
    "\n",
    "            import pickle as pkl \n",
    "            metaData_Dictionary = {}\n",
    "            metaData_Dictionary['norm'] =  norm\n",
    "            metaData_Dictionary['Features'] =  dl._model_json['output']['names']\n",
    "            with open('Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+str(epoch)+'.pkl', 'wb') as file:\n",
    "                pkl.dump(metaData_Dictionary, file)\n",
    "\n",
    "            model.save('Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+str(epoch)+'.hdf5')\n",
    "            import h2o\n",
    "            import pandas as pd\n",
    "\n",
    "            # Assuming you have an H2O model named 'model'\n",
    "\n",
    "            # Get the scoring history from the model\n",
    "            scoring_history = dl.scoring_history()\n",
    "\n",
    "            # Extract the training and validation logloss\n",
    "            training_logloss = scoring_history['training_logloss']\n",
    "            validation_logloss = scoring_history['validation_logloss']\n",
    "\n",
    "            # Create a DataFrame to store the logloss values\n",
    "            logloss_df = pd.DataFrame({'Training Logloss': training_logloss, 'Validation Logloss': validation_logloss})\n",
    "\n",
    "            # Save the logloss values to a CSV file\n",
    "            logloss_df.to_csv('Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+str(epoch)+'model'+'.csv', index=False)\n",
    "            \n",
    "            training_auc = scoring_history['training_auc']\n",
    "            validation_auc = scoring_history['validation_auc']\n",
    "\n",
    "            # Create a DataFrame to store the AUC values\n",
    "            auc_df = pd.DataFrame({'Training AUC': training_auc, 'Validation AUC': validation_auc})\n",
    "\n",
    "            # Save the AUC values to a CSV file\n",
    "            auc_df.to_csv('Hidden' + str(FirstLayer) + '_' + str(SecondLayer) +'Epoch'+str(epoch)+ '_AUC.csv', index=False)\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Create a new figure\n",
    "            # plt.figure()\n",
    "\n",
    "            # Generate the plot using dl.plot()\n",
    "            # dl.plot()\n",
    "\n",
    "            # Save the plot with high resolution\n",
    "            # plt.savefig('LearningRate'+'Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+str(epoch)+'model'+'.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3af38a-22c4-4591-a3ea-65eba8e33876",
   "metadata": {},
   "source": [
    "# Regression : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d04f21b-e5a3-4f66-b267-1e7fa407009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTensor = train.as_data_frame()\n",
    "validTensor = valid.as_data_frame()\n",
    "testTensor  = test.as_data_frame()\n",
    "trainTensorY = trainTensor[target]\n",
    "trainTensor = trainTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "validTensorY = validTensor[target]\n",
    "validTensor = validTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm\n",
    "testTensorY = testTensor[target]\n",
    "testTensor = testTensor[dl._model_json['output']['names']].iloc[::,:-1]*norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41a24e52-c03f-43c6-a83a-d9bf4f5feaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "350/350 - 1s - loss: 0.3337 - accuracy: 0.8572 - val_loss: 0.2413 - val_accuracy: 0.9016\n",
      "Epoch 2/2\n",
      "350/350 - 0s - loss: 0.2246 - accuracy: 0.9125 - val_loss: 0.2145 - val_accuracy: 0.9168\n",
      "102/102 - 0s - loss: 0.2265 - accuracy: 0.9103\n",
      "Test accuracy: 0.9103193879127502\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0)\n",
    "\n",
    "\n",
    "model.fit(trainTensor, trainTensorY, epochs=2,verbose=2, validation_data=(validTensor, validTensorY))\n",
    "\n",
    "# Evaluate your model\n",
    "test_loss, test_acc = model.evaluate(testTensor, testTensorY, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecbeac61-7deb-4bc0-b953-31dcc79108e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 - 0s - loss: 0.2265 - accuracy: 0.9103\n",
      "Test accuracy: 0.9103193879127502\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testTensor, testTensorY, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad503ab-ff03-4509-9c57-bb5bf7b3afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4373d-1ab3-4816-ad28-2e9e6fa711f9",
   "metadata": {},
   "source": [
    "### Test The H2O model vs TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbaed144-760e-4075-b0fa-964b876aa254",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9972378  0.00276219]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.13082215 0.8691778 ]]\n",
      "H20 pred [1] tensor_pred 1 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.92598075 0.07401925]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.7457433  0.25425673]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9150704  0.08492957]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.97998327 0.02001668]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9944588 0.0055412]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.8107615 0.1892385]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.970182   0.02981793]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.86912036 0.13087964]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.16695929 0.83304065]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.8591717  0.14082831]]\n",
      "H20 pred [0] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9952396  0.00476034]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.00315079 0.99684924]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98704755 0.01295249]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9944299  0.00557014]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9941561 0.0058439]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.05264694 0.9473531 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9933956  0.00660441]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9758657  0.02413424]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9954484  0.00455154]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.04478607 0.95521384]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.99714065 0.00285933]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.998381 0.001619]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9940606  0.00593939]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.813614   0.18638596]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.81517375 0.18482628]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.4359093  0.56409067]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.8445797  0.15542027]]\n",
      "H20 pred [0] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.02000849 0.97999156]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9964965  0.00350357]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.04031136 0.95968866]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9890609  0.01093908]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.2677765 0.7322236]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98175275 0.0182472 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.96868336 0.03131666]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98544043 0.0145596 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.0641462  0.93585384]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.7352318 0.2647682]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.03132886 0.96867114]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98388284 0.01611711]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.6814671  0.31853294]]\n",
      "H20 pred [1] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.47608525 0.5239147 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.859834   0.14016598]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9531789  0.04682114]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.757645   0.24235493]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.994784e-01 5.216353e-04]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.58851063 0.4114893 ]]\n",
      "H20 pred [1] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.93902487 0.06097514]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.03485915 0.9651409 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.81594175 0.1840583 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.03978366 0.9602163 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.99824774 0.00175232]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9845613  0.01543866]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.35678387 0.6432161 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.26392907 0.73607093]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.3867123e-04 9.9906129e-01]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.0039014  0.99609864]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.020216 0.979784]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.02344741 0.97655255]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.30960763 0.6903924 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.96179634 0.0382036 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.94134694 0.05865306]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9129871  0.08701284]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9963121  0.00368796]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.03360056 0.96639943]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.41494575 0.5850542 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.42281148 0.57718855]]\n",
      "H20 pred [0] tensor_pred 1 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.01805067 0.9819494 ]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.93285626 0.06714381]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.96931195 0.03068803]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.7764681 0.2235319]]\n",
      "H20 pred [0] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.25340897 0.74659103]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.4387826  0.56121737]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98440105 0.01559898]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.8084344  0.19156565]]\n",
      "H20 pred [1] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.6880189  0.31198108]]\n",
      "H20 pred [1] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.92439425 0.07560576]]\n",
      "H20 pred [1] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9799009  0.02009914]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.6797611  0.32023892]]\n",
      "H20 pred [1] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9351965  0.06480345]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.72718996 0.27281004]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98444635 0.01555363]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.9932075e-01 6.7930407e-04]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.87773454 0.12226546]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9818409  0.01815914]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.8310562  0.16894381]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9684387  0.03156137]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.9938405e-01 6.1590754e-04]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9949167  0.00508328]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.9956375e-01 4.3620940e-04]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.99386394 0.0061361 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.92108047 0.07891951]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[9.9928266e-01 7.1741419e-04]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.0280647 0.9719353]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.7299264  0.27007353]]\n",
      "H20 pred [1] tensor_pred 0 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.93957    0.06042999]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.98973364 0.0102664 ]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.00441919 0.99558085]]\n",
      "H20 pred [1] tensor_pred 1 y 1\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[[0.9864194  0.01358058]]\n",
      "H20 pred [0] tensor_pred 0 y 0\n"
     ]
    }
   ],
   "source": [
    "Which = 1\n",
    "for Which in range(0,100):\n",
    "    XforH2o = data[dl._model_json['output']['names']][Which,:-1]\n",
    "    pred = dl.predict(XforH2o)\n",
    "    x = data[dl._model_json['output']['names']][Which,:-1].as_data_frame()\n",
    "    #print(x)\n",
    "    x = pd.DataFrame(x.values[0]*norm).T\n",
    "    #print(x.T)\\\n",
    "    y = data[dl._model_json['output']['names']][Which,-1]\n",
    "    y_pred = model.predict(x)\n",
    "    print(y_pred)\n",
    "    #print(pred[['p0']])\n",
    "    y_pred_class = np.argmax(y_pred)\n",
    "    print('H20 pred',pred['predict'].as_data_frame().values[0],'tensor_pred',y_pred_class,'y',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dbd50-3c42-48b7-8df4-4930414b9f02",
   "metadata": {},
   "source": [
    "# Saving the Meta-Data and HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dcc9dec-dcda-4f8a-95a0-18f28bb4945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "metaData_Dictionary = {}\n",
    "metaData_Dictionary['norm'] =  norm\n",
    "metaData_Dictionary['Features'] =  dl._model_json['output']['names']\n",
    "with open('Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+'10'+'.pkl', 'wb') as file:\n",
    "    pkl.dump(metaData_Dictionary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fd46bde-ca41-4279-be21-a31353401edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('metaData.pkl', 'rb') as file:\n",
    "#     myvar = pkl.load(file)\n",
    "#     print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "337c5333-1776-46b2-ac34-ee837ec4cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Hidden'+str(FirstLayer)+'_'+str(SecondLayer)+'Epoch'+'10'+'.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd21f3-922e-4e76-96ad-48b660dce4c1",
   "metadata": {},
   "source": [
    "### Valdation VS. Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083fc52c-4a5a-4fe6-a680-f25f9ed015f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have an H2O model named 'model'\n",
    "\n",
    "# Get the scoring history from the model\n",
    "scoring_history = dl.scoring_history()\n",
    "\n",
    "# Extract the training and validation logloss\n",
    "training_logloss = scoring_history['training_logloss']\n",
    "validation_logloss = scoring_history['validation_logloss']\n",
    "\n",
    "# Create a DataFrame to store the logloss values\n",
    "logloss_df = pd.DataFrame({'Training Logloss': training_logloss, 'Validation Logloss': validation_logloss})\n",
    "\n",
    "# Save the logloss values to a CSV file\n",
    "logloss_df.to_csv('logloss.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "710fb219-d9c5-4d20-9197-fae68e3969c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure()\n",
    "\n",
    "# Generate the plot using dl.plot()\n",
    "dl.plot()\n",
    "\n",
    "# Save the plot with high resolution\n",
    "plt.savefig('LearningRate.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aee285-015e-4395-a4e0-59443ca70c73",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "04e5d205-6947-482b-99f7-1230566c77e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable  relative_importance  scaled_importance  percentage\n",
      "0    CIR018             1.000000           1.000000    0.006301\n",
      "1    ESA005             0.752320           0.752320    0.004741\n",
      "2    GEN002             0.719683           0.719683    0.004535\n",
      "3    INF002             0.623220           0.623220    0.003927\n",
      "4    SYM003             0.617477           0.617477    0.003891\n",
      "5    NVS020             0.594421           0.594421    0.003746\n",
      "6    NEO070             0.567286           0.567286    0.003575\n",
      "7    DIG018             0.550291           0.550291    0.003468\n",
      "8    CIR007             0.504869           0.504869    0.003181\n",
      "9    ESA004             0.481697           0.481697    0.003035\n",
      "10   RSP010             0.474172           0.474172    0.002988\n",
      "11   END011             0.467395           0.467395    0.002945\n",
      "12   HEP005             0.459627           0.459627    0.002896\n",
      "13   HEP008             0.459507           0.459507    0.002896\n",
      "14   ADM006             0.454762           0.454762    0.002866\n",
      "15   END010             0.451075           0.451075    0.002842\n",
      "16   NVS013             0.442237           0.442237    0.002787\n",
      "17   HEP001             0.441672           0.441672    0.002783\n",
      "18   DIG004             0.434019           0.434019    0.002735\n",
      "19   HEP002             0.425473           0.425473    0.002681\n",
      "20   SYM016             0.416931           0.416931    0.002627\n",
      "21   FAC025             0.406112           0.406112    0.002559\n",
      "22   CAR024             0.397209           0.397209    0.002503\n",
      "23      age             0.393410           0.393410    0.002479\n",
      "24   SYM006             0.390323           0.390323    0.002460\n",
      "25   XXX000             0.390002           0.390002    0.002458\n",
      "26   BLD003             0.389045           0.389045    0.002452\n",
      "27   female             0.385745           0.385745    0.002431\n",
      "28   END008             0.384743           0.384743    0.002424\n",
      "29   BLD006             0.383616           0.383616    0.002417\n"
     ]
    }
   ],
   "source": [
    "feat_imp = dl.varimp(use_pandas=True)\n",
    "print(feat_imp.head(30))\n",
    "feat_imp.to_csv('Died_Fet_importance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab43738-3a88-4fc1-afb2-5dfb4b4a806a",
   "metadata": {},
   "source": [
    "### Name Of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd4d1b2d-aae8-4f45-be59-98bea3a730a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesi = data[dl._model_json['output']['names']][Which,:-1]\n",
    "#data[dl._model_json['output']['names']][Which,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10f9dd76-9a32-4a54-b7d9-e76ab7f0b234",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age    race2.0    race6.0    race5.0    race4.0    race3.0    race0.0    race1.0    female    BLD001    BLD002    BLD003    BLD004    BLD005    BLD006    BLD007    BLD008    BLD009    BLD010    CIR001    CIR002    CIR003    CIR004    CIR005    CIR006    CIR007    CIR008    CIR009    CIR010    CIR011    CIR012    CIR013    CIR014    CIR015    CIR016    CIR017    CIR018    CIR019    CIR020    CIR021    CIR022    CIR023    CIR024    CIR025    CIR026    CIR027    CIR028    CIR029    CIR030    CIR031    CIR032    CIR033    CIR034    CIR035    CIR036    CIR038    CIR039    DIG001    DIG002    DIG003    DIG004    DIG005    DIG006    DIG007    DIG008    DIG009    DIG010    DIG011    DIG012    DIG013    DIG014    DIG015    DIG016    DIG017    DIG018    DIG019    DIG020    DIG021    DIG022    DIG023    DIG024    DIG025    EAR002    EAR003    EAR004    EAR006    END001    END002    END003    END007    END008    END009    END010    END011    END012    END013    END014    END015    END016    EYE001    EYE002    EYE003    EYE004    EYE005    EYE006    EYE007    EYE008    EYE009    EYE010    EYE012    FAC003    FAC004    FAC006    FAC008    FAC009    FAC014    FAC016    FAC020    FAC021    FAC022    FAC025    GEN001    GEN002    GEN003    GEN004    GEN005    GEN006    GEN007    GEN008    GEN009    GEN010    GEN012    GEN013    GEN014    GEN016    GEN017    GEN018    GEN019    GEN020    GEN021    GEN022    GEN023    GEN025    GEN026    INF001    INF002    INF003    INF004    INF005    INF006    INF007    INF008    INF009    INF010    INJ001    INJ002    INJ003    INJ004    INJ005    INJ006    INJ007    INJ008    INJ009    INJ010    INJ011    INJ012    INJ013    INJ016    INJ017    INJ019    INJ020    INJ021    INJ022    INJ023    INJ024    INJ025    INJ026    INJ027    INJ028    INJ030    INJ031    INJ033    INJ034    INJ035    INJ036    INJ037    INJ039    INJ040    INJ041    INJ042    INJ043    INJ045    INJ047    INJ049    INJ050    INJ054    INJ069    INJ072    INJ073    INJ074    INJ076    MAL001    MAL002    MAL003    MAL004    MAL008    MAL009    MAL010    MBD001    MBD002    MBD003    MBD004    MBD005    MBD006    MBD007    MBD008    MBD009    MBD010    MBD011    MBD012    MBD013    MBD014    MBD017    MBD018    MBD019    MBD020    MBD021    MBD022    MBD024    MBD025    MBD026    MUS001    MUS002    MUS003    MUS004    MUS006    MUS007    MUS008    MUS009    MUS010    MUS011    MUS012    MUS013    MUS014    MUS015    MUS016    MUS017    MUS020    MUS021    MUS022    MUS023    MUS024    MUS025    MUS026    MUS028    MUS029    MUS030    MUS032    MUS033    MUS034    MUS037    MUS038    NEO002    NEO007    NEO008    NEO009    NEO010    NEO012    NEO013    NEO014    NEO015    NEO016    NEO017    NEO018    NEO019    NEO020    NEO021    NEO022    NEO023    NEO024    NEO025    NEO026    NEO027    NEO028    NEO029    NEO030    NEO031    NEO032    NEO033    NEO035    NEO036    NEO039    NEO040    NEO043    NEO044    NEO045    NEO047    NEO050    NEO056    NEO057    NEO058    NEO059    NEO060    NEO061    NEO062    NEO063    NEO064    NEO065    NEO066    NEO067    NEO068    NEO069    NEO070    NEO071    NEO072    NEO073    NEO074    NVS001    NVS002    NVS003    NVS004    NVS005    NVS006    NVS007    NVS008    NVS009    NVS010    NVS011    NVS012    NVS013    NVS014    NVS015    NVS016    NVS017    NVS018    NVS019    NVS020    NVS021    NVS022    RSP001    RSP002    RSP003    RSP005    RSP006    RSP007    RSP008    RSP009    RSP010    RSP011    RSP013    RSP014    RSP015    RSP016    RSP017    SKN001    SKN002    SKN003    SKN004    SKN005    SKN006    SKN007    SYM001    SYM002    SYM003    SYM004    SYM005    SYM006    SYM007    SYM008    SYM010    SYM011    SYM012    SYM013    SYM014    SYM015    SYM016    SYM017    XXX000    ADM001    ADM002    ADM003    ADM004    ADM005    ADM006    ADM007    ADM010    ADM012    ADM013    ADM014    ADM015    ADM016    ADM017    ADM018    ADM020    ADM021    CAR002    CAR003    CAR004    CAR006    CAR007    CAR008    CAR009    CAR010    CAR011    CAR012    CAR014    CAR015    CAR016    CAR017    CAR019    CAR020    CAR021    CAR022    CAR023    CAR024    CAR025    CAR026    CAR028    CAR029    CNS001    CNS002    CNS004    CNS005    CNS006    CNS007    CNS008    CNS010    CNS011    CNS013    CNS014    ENP001    ENP004    ENT001    ENT002    ENT003    ENT006    ENT013    ENT017    ESA001    ESA002    ESA004    ESA005    ESA006    ESA007    ESA008    ESA009    ESA010    EST001    EST004    EST005    FRS001    FRS002    FRS003    FRS004    FRS010    FRS015    GIS001    GIS002    GIS003    GIS004    GIS005    GIS006    GIS007    GIS008    GIS009    GIS010    GIS011    GIS012    GIS013    GIS015    GIS016    GIS017    GIS018    GIS019    GIS021    GIS022    GIS024    GIS025    GIS026    GIS027    GIS028    GIS029    GNR002    GNR003    GNR004    GNR005    GNR006    GNR007    GNR008    GNR009    HEP001    HEP002    HEP003    HEP004    HEP005    HEP006    HEP007    HEP008    HEP009    HEP010    HEP013    IMG001    IMG002    IMG003    IMG004    IMG005    IMG006    IMG007    IMG008    IMG009    LYM001    LYM002    LYM003    LYM005    LYM006    LYM007    MAM001    MAM002    MAM003    MAM004    MAM005    MAM006    MAM007    MAM008    MAM009    MAM010    MAM011    MAM013    MAM015    MRS003    MRS004    MRS005    MST001    MST002    MST003    MST004    MST005    MST006    MST007    MST008    MST009    MST010    MST011    MST012    MST013    MST014    MST015    MST016    MST017    MST018    MST019    MST020    MST022    MST023    MST024    MST025    MST028    MST029    MST030    NCM001    NCM002    OST001    OTR001    OTR002    OTR004    PLC001    PLC002    PNS001    PNS002    PNS005    PNS006    RAD001    RAD002    RAD003    RAD004    RES001    RES002    RES003    RES004    RES005    RES006    RES008    RES009    RES010    RES011    RES012    RES014    RHB001    RHB002    RHB004    SKB001    SKB002    SKB006    SKB007    SKB008    SKB009    SKB010    SUD001    SUD002    URN001    URN002    URN003    URN004    URN005    URN006    URN008    URN009    URN010\n",
      "   76          0          0          0          0          0          1          0         0         0         0         0         0         0         1         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         1         1         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         1         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         1         0         0         1         0         0         0         0         0         0         0         0         0         0         1         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         1         0         0         0         0         0         0         0         1         0         1         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0\n",
      "[1 row x 608 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tesi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4bed0-b6fe-4bb4-af34-453d5f396ac2",
   "metadata": {},
   "source": [
    "### Performance On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a89f852a-6bb4-42d7-b1e0-35a7292a0c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: deeplearning\n",
       "** Reported on test data. **\n",
       "\n",
       "MSE: 0.08257808201709212\n",
       "RMSE: 0.2873640235260707\n",
       "LogLoss: 0.27580436001178554\n",
       "Mean Per-Class Error: 0.112748505811638\n",
       "AUC: 0.952339129381051\n",
       "AUCPR: 0.9319101463820745\n",
       "Gini: 0.9046782587621021</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-17.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-17 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-17 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-17 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-17 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-17 .h2o-table th,\n",
       "#h2o-table-17 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-17 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-17\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4721037834872541</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>0</th>\n",
       "<th>1</th>\n",
       "<th>Error</th>\n",
       "<th>Rate</th></tr></thead>\n",
       "    <tbody><tr><td>0</td>\n",
       "<td>2602.0</td>\n",
       "<td>272.0</td>\n",
       "<td>0.0946</td>\n",
       "<td> (272.0/2874.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>257.0</td>\n",
       "<td>1707.0</td>\n",
       "<td>0.1309</td>\n",
       "<td> (257.0/1964.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2859.0</td>\n",
       "<td>1979.0</td>\n",
       "<td>0.1093</td>\n",
       "<td> (529.0/4838.0)</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-18.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-18 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-18 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-18 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-18 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-18 .h2o-table th,\n",
       "#h2o-table-18 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-18 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-18\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
       "    <thead><tr><th>metric</th>\n",
       "<th>threshold</th>\n",
       "<th>value</th>\n",
       "<th>idx</th></tr></thead>\n",
       "    <tbody><tr><td>max f1</td>\n",
       "<td>0.4721038</td>\n",
       "<td>0.8658382</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2527399</td>\n",
       "<td>0.8991441</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6660238</td>\n",
       "<td>0.8847138</td>\n",
       "<td>144.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5467759</td>\n",
       "<td>0.8914841</td>\n",
       "<td>178.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9951461</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0094198</td>\n",
       "<td>1.0</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9951461</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5183321</td>\n",
       "<td>0.7741448</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4244212</td>\n",
       "<td>0.8828921</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4721038</td>\n",
       "<td>0.8872515</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9951461</td>\n",
       "<td>2874.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9951461</td>\n",
       "<td>1961.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0030286</td>\n",
       "<td>2874.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0094198</td>\n",
       "<td>1964.0</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9951461</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9951461</td>\n",
       "<td>0.9984725</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0030286</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0094198</td>\n",
       "<td>1.0</td>\n",
       "<td>393.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-19.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-19 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-19 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-19 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-19 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-19 .h2o-table th,\n",
       "#h2o-table-19 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-19 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-19\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Gains/Lift Table: Avg response rate: 40.60 %, avg score: 40.99 %</caption>\n",
       "    <thead><tr><th>group</th>\n",
       "<th>cumulative_data_fraction</th>\n",
       "<th>lower_threshold</th>\n",
       "<th>lift</th>\n",
       "<th>cumulative_lift</th>\n",
       "<th>response_rate</th>\n",
       "<th>score</th>\n",
       "<th>cumulative_response_rate</th>\n",
       "<th>cumulative_score</th>\n",
       "<th>capture_rate</th>\n",
       "<th>cumulative_capture_rate</th>\n",
       "<th>gain</th>\n",
       "<th>cumulative_gain</th>\n",
       "<th>kolmogorov_smirnov</th></tr></thead>\n",
       "    <tbody><tr><td>1</td>\n",
       "<td>0.0101282</td>\n",
       "<td>0.9878881</td>\n",
       "<td>2.4633401</td>\n",
       "<td>2.4633401</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9895468</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9895468</td>\n",
       "<td>0.0249491</td>\n",
       "<td>0.0249491</td>\n",
       "<td>146.3340122</td>\n",
       "<td>146.3340122</td>\n",
       "<td>0.0249491</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200496</td>\n",
       "<td>0.9867772</td>\n",
       "<td>2.4120205</td>\n",
       "<td>2.4379449</td>\n",
       "<td>0.9791667</td>\n",
       "<td>0.9873296</td>\n",
       "<td>0.9896907</td>\n",
       "<td>0.9884496</td>\n",
       "<td>0.0239308</td>\n",
       "<td>0.0488798</td>\n",
       "<td>141.2020536</td>\n",
       "<td>143.7944863</td>\n",
       "<td>0.0485319</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0301778</td>\n",
       "<td>0.9852741</td>\n",
       "<td>2.4130679</td>\n",
       "<td>2.4295957</td>\n",
       "<td>0.9795918</td>\n",
       "<td>0.9861073</td>\n",
       "<td>0.9863014</td>\n",
       "<td>0.9876635</td>\n",
       "<td>0.0244399</td>\n",
       "<td>0.0733198</td>\n",
       "<td>141.3067875</td>\n",
       "<td>142.9595737</td>\n",
       "<td>0.0726239</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400992</td>\n",
       "<td>0.9834888</td>\n",
       "<td>2.4633401</td>\n",
       "<td>2.4379449</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9844051</td>\n",
       "<td>0.9896907</td>\n",
       "<td>0.9868573</td>\n",
       "<td>0.0244399</td>\n",
       "<td>0.0977597</td>\n",
       "<td>146.3340122</td>\n",
       "<td>143.7944863</td>\n",
       "<td>0.0970638</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500207</td>\n",
       "<td>0.9813125</td>\n",
       "<td>2.4120205</td>\n",
       "<td>2.4328028</td>\n",
       "<td>0.9791667</td>\n",
       "<td>0.9823802</td>\n",
       "<td>0.9876033</td>\n",
       "<td>0.9859693</td>\n",
       "<td>0.0239308</td>\n",
       "<td>0.1216904</td>\n",
       "<td>141.2020536</td>\n",
       "<td>143.2802848</td>\n",
       "<td>0.1206466</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000413</td>\n",
       "<td>0.9632344</td>\n",
       "<td>2.3920865</td>\n",
       "<td>2.4124447</td>\n",
       "<td>0.9710744</td>\n",
       "<td>0.9729685</td>\n",
       "<td>0.9793388</td>\n",
       "<td>0.9794689</td>\n",
       "<td>0.1196538</td>\n",
       "<td>0.2413442</td>\n",
       "<td>139.2086482</td>\n",
       "<td>141.2444665</td>\n",
       "<td>0.2378647</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500620</td>\n",
       "<td>0.9386229</td>\n",
       "<td>2.3717283</td>\n",
       "<td>2.3988725</td>\n",
       "<td>0.9628099</td>\n",
       "<td>0.9506723</td>\n",
       "<td>0.9738292</td>\n",
       "<td>0.9698700</td>\n",
       "<td>0.1186354</td>\n",
       "<td>0.3599796</td>\n",
       "<td>137.1728299</td>\n",
       "<td>139.8872543</td>\n",
       "<td>0.3533686</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000827</td>\n",
       "<td>0.9009408</td>\n",
       "<td>2.3615492</td>\n",
       "<td>2.3895417</td>\n",
       "<td>0.9586777</td>\n",
       "<td>0.9214336</td>\n",
       "<td>0.9700413</td>\n",
       "<td>0.9577609</td>\n",
       "<td>0.1181263</td>\n",
       "<td>0.4781059</td>\n",
       "<td>136.1549208</td>\n",
       "<td>138.9541709</td>\n",
       "<td>0.4680154</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3001240</td>\n",
       "<td>0.7562547</td>\n",
       "<td>2.1325197</td>\n",
       "<td>2.3038677</td>\n",
       "<td>0.8657025</td>\n",
       "<td>0.8401356</td>\n",
       "<td>0.9352617</td>\n",
       "<td>0.9185525</td>\n",
       "<td>0.2133401</td>\n",
       "<td>0.6914460</td>\n",
       "<td>113.2519651</td>\n",
       "<td>130.3867690</td>\n",
       "<td>0.6587390</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.3999587</td>\n",
       "<td>0.5022385</td>\n",
       "<td>1.6524269</td>\n",
       "<td>2.1412600</td>\n",
       "<td>0.6708075</td>\n",
       "<td>0.6379856</td>\n",
       "<td>0.8692506</td>\n",
       "<td>0.8485195</td>\n",
       "<td>0.1649695</td>\n",
       "<td>0.8564155</td>\n",
       "<td>65.2426914</td>\n",
       "<td>114.1259993</td>\n",
       "<td>0.7683849</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.2550888</td>\n",
       "<td>0.8346855</td>\n",
       "<td>1.8798371</td>\n",
       "<td>0.3388430</td>\n",
       "<td>0.3673137</td>\n",
       "<td>0.7631253</td>\n",
       "<td>0.7522386</td>\n",
       "<td>0.0835031</td>\n",
       "<td>0.9399185</td>\n",
       "<td>-16.5314504</td>\n",
       "<td>87.9837067</td>\n",
       "<td>0.7405448</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.6000413</td>\n",
       "<td>0.1210911</td>\n",
       "<td>0.3409996</td>\n",
       "<td>1.6232758</td>\n",
       "<td>0.1384298</td>\n",
       "<td>0.1785882</td>\n",
       "<td>0.6589735</td>\n",
       "<td>0.6565972</td>\n",
       "<td>0.0341141</td>\n",
       "<td>0.9740326</td>\n",
       "<td>-65.9000438</td>\n",
       "<td>62.3275802</td>\n",
       "<td>0.6295649</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.6998760</td>\n",
       "<td>0.0594614</td>\n",
       "<td>0.1377022</td>\n",
       "<td>1.4113644</td>\n",
       "<td>0.0559006</td>\n",
       "<td>0.0877515</td>\n",
       "<td>0.5729474</td>\n",
       "<td>0.5754536</td>\n",
       "<td>0.0137475</td>\n",
       "<td>0.9877800</td>\n",
       "<td>-86.2297757</td>\n",
       "<td>41.1364394</td>\n",
       "<td>0.4846485</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.7999173</td>\n",
       "<td>0.0288216</td>\n",
       "<td>0.0916118</td>\n",
       "<td>1.2463101</td>\n",
       "<td>0.0371901</td>\n",
       "<td>0.0433622</td>\n",
       "<td>0.5059432</td>\n",
       "<td>0.5089078</td>\n",
       "<td>0.0091650</td>\n",
       "<td>0.9969450</td>\n",
       "<td>-90.8388177</td>\n",
       "<td>24.6310067</td>\n",
       "<td>0.3316701</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999587</td>\n",
       "<td>0.0134437</td>\n",
       "<td>0.0254477</td>\n",
       "<td>1.1105964</td>\n",
       "<td>0.0103306</td>\n",
       "<td>0.0200349</td>\n",
       "<td>0.4508498</td>\n",
       "<td>0.4545636</td>\n",
       "<td>0.0025458</td>\n",
       "<td>0.9994908</td>\n",
       "<td>-97.4552271</td>\n",
       "<td>11.0596385</td>\n",
       "<td>0.1675493</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0003835</td>\n",
       "<td>0.0050895</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0020661</td>\n",
       "<td>0.0085922</td>\n",
       "<td>0.4059529</td>\n",
       "<td>0.4099480</td>\n",
       "<td>0.0005092</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.4910454</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>"
      ],
      "text/plain": [
       "ModelMetricsBinomial: deeplearning\n",
       "** Reported on test data. **\n",
       "\n",
       "MSE: 0.08257808201709212\n",
       "RMSE: 0.2873640235260707\n",
       "LogLoss: 0.27580436001178554\n",
       "Mean Per-Class Error: 0.112748505811638\n",
       "AUC: 0.952339129381051\n",
       "AUCPR: 0.9319101463820745\n",
       "Gini: 0.9046782587621021\n",
       "\n",
       "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4721037834872541\n",
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  --------------\n",
       "0      2602  272   0.0946   (272.0/2874.0)\n",
       "1      257   1707  0.1309   (257.0/1964.0)\n",
       "Total  2859  1979  0.1093   (529.0/4838.0)\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.472104     0.865838  200\n",
       "max f2                       0.25274      0.899144  268\n",
       "max f0point5                 0.666024     0.884714  144\n",
       "max accuracy                 0.546776     0.891484  178\n",
       "max precision                0.995146     1         0\n",
       "max recall                   0.00941984   1         393\n",
       "max specificity              0.995146     1         0\n",
       "max absolute_mcc             0.518332     0.774145  187\n",
       "max min_per_class_accuracy   0.424421     0.882892  215\n",
       "max mean_per_class_accuracy  0.472104     0.887251  200\n",
       "max tns                      0.995146     2874      0\n",
       "max fns                      0.995146     1961      0\n",
       "max fps                      0.00302859   2874      399\n",
       "max tps                      0.00941984   1964      393\n",
       "max tnr                      0.995146     1         0\n",
       "max fnr                      0.995146     0.998473  0\n",
       "max fpr                      0.00302859   1         399\n",
       "max tpr                      0.00941984   1         393\n",
       "\n",
       "Gains/Lift Table: Avg response rate: 40.60 %, avg score: 40.99 %\n",
       "group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0101282                   0.987888           2.46334     2.46334            1                0.989547    1                           0.989547            0.0249491       0.0249491                  146.334   146.334            0.0249491\n",
       "2        0.0200496                   0.986777           2.41202     2.43794            0.979167         0.98733     0.989691                    0.98845             0.0239308       0.0488798                  141.202   143.794            0.0485319\n",
       "3        0.0301778                   0.985274           2.41307     2.4296             0.979592         0.986107    0.986301                    0.987664            0.0244399       0.0733198                  141.307   142.96             0.0726239\n",
       "4        0.0400992                   0.983489           2.46334     2.43794            1                0.984405    0.989691                    0.986857            0.0244399       0.0977597                  146.334   143.794            0.0970638\n",
       "5        0.0500207                   0.981312           2.41202     2.4328             0.979167         0.98238     0.987603                    0.985969            0.0239308       0.12169                    141.202   143.28             0.120647\n",
       "6        0.100041                    0.963234           2.39209     2.41244            0.971074         0.972969    0.979339                    0.979469            0.119654        0.241344                   139.209   141.244            0.237865\n",
       "7        0.150062                    0.938623           2.37173     2.39887            0.96281          0.950672    0.973829                    0.96987             0.118635        0.35998                    137.173   139.887            0.353369\n",
       "8        0.200083                    0.900941           2.36155     2.38954            0.958678         0.921434    0.970041                    0.957761            0.118126        0.478106                   136.155   138.954            0.468015\n",
       "9        0.300124                    0.756255           2.13252     2.30387            0.865702         0.840136    0.935262                    0.918552            0.21334         0.691446                   113.252   130.387            0.658739\n",
       "10       0.399959                    0.502239           1.65243     2.14126            0.670807         0.637986    0.869251                    0.84852             0.164969        0.856415                   65.2427   114.126            0.768385\n",
       "11       0.5                         0.255089           0.834685    1.87984            0.338843         0.367314    0.763125                    0.752239            0.0835031       0.939919                   -16.5315  87.9837            0.740545\n",
       "12       0.600041                    0.121091           0.341       1.62328            0.13843          0.178588    0.658973                    0.656597            0.0341141       0.974033                   -65.9     62.3276            0.629565\n",
       "13       0.699876                    0.0594614          0.137702    1.41136            0.0559006        0.0877515   0.572947                    0.575454            0.0137475       0.98778                    -86.2298  41.1364            0.484649\n",
       "14       0.799917                    0.0288216          0.0916118   1.24631            0.0371901        0.0433622   0.505943                    0.508908            0.00916497      0.996945                   -90.8388  24.631             0.33167\n",
       "15       0.899959                    0.0134437          0.0254477   1.1106             0.0103306        0.0200349   0.45085                     0.454564            0.00254582      0.999491                   -97.4552  11.0596            0.167549\n",
       "16       1                           0.000383453        0.00508955  1                  0.00206612       0.00859217  0.405953                    0.409948            0.000509165     1                          -99.491   0                  0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = dl.model_performance(test)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e4f6cf9-11a0-470e-a15d-ba1dc3f80c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Assuming that `perf` contains the model performance metrics\n",
    "auc = perf.auc()\n",
    "fprs,tprs = perf.roc()\n",
    "\n",
    "# Create a Scatter trace with x=fprs, y=tprs, and show the AUC in the hover text\n",
    "trace = go.Scatter(\n",
    "    x=fprs,\n",
    "    y=tprs,\n",
    "    mode=\"lines\",\n",
    "    hovertemplate=\"False Positive Rate: %{x:.2f}<br>True Positive Rate: %{y:.2f}<br>AUC: %{text:.3f}\",\n",
    "    text=[auc]*len(fprs)\n",
    ")\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    title=\"ROC Curve\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\"\n",
    ")\n",
    "\n",
    "# Create a Figure object and add the trace and layout\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "fig.write_html(\"roc_curve.html\")\n",
    "\n",
    "# Display the plot\n",
    "#fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76277cc2-6897-4995-9d61-01e70a2843b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790658046703884"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d976570-9af4-41a7-ba72-0ac720673b89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0005141388174807198,\n",
       "  0.0005141388174807198,\n",
       "  0.0005141388174807198,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0010282776349614395,\n",
       "  0.0015424164524421595,\n",
       "  0.002056555269922879,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.002570694087403599,\n",
       "  0.003084832904884319,\n",
       "  0.003084832904884319,\n",
       "  0.0035989717223650387,\n",
       "  0.0035989717223650387,\n",
       "  0.0035989717223650387,\n",
       "  0.0035989717223650387,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004113110539845758,\n",
       "  0.004627249357326478,\n",
       "  0.004627249357326478,\n",
       "  0.005141388174807198,\n",
       "  0.005655526992287918,\n",
       "  0.006169665809768638,\n",
       "  0.007712082262210797,\n",
       "  0.008226221079691516,\n",
       "  0.008226221079691516,\n",
       "  0.008226221079691516,\n",
       "  0.008226221079691516,\n",
       "  0.008740359897172237,\n",
       "  0.009768637532133676,\n",
       "  0.009768637532133676,\n",
       "  0.010282776349614395,\n",
       "  0.010282776349614395,\n",
       "  0.010282776349614395,\n",
       "  0.010282776349614395,\n",
       "  0.010282776349614395,\n",
       "  0.010282776349614395,\n",
       "  0.010796915167095116,\n",
       "  0.011311053984575836,\n",
       "  0.011311053984575836,\n",
       "  0.011825192802056555,\n",
       "  0.012339331619537276,\n",
       "  0.012339331619537276,\n",
       "  0.012853470437017995,\n",
       "  0.012853470437017995,\n",
       "  0.012853470437017995,\n",
       "  0.013367609254498715,\n",
       "  0.013367609254498715,\n",
       "  0.013881748071979434,\n",
       "  0.014395886889460155,\n",
       "  0.014910025706940874,\n",
       "  0.015938303341902313,\n",
       "  0.015938303341902313,\n",
       "  0.016452442159383032,\n",
       "  0.016452442159383032,\n",
       "  0.01696658097686375,\n",
       "  0.01696658097686375,\n",
       "  0.01696658097686375,\n",
       "  0.018508997429305913,\n",
       "  0.018508997429305913,\n",
       "  0.019023136246786632,\n",
       "  0.01953727506426735,\n",
       "  0.02056555269922879,\n",
       "  0.02056555269922879,\n",
       "  0.02056555269922879,\n",
       "  0.02056555269922879,\n",
       "  0.02056555269922879,\n",
       "  0.02056555269922879,\n",
       "  0.021079691516709513,\n",
       "  0.021079691516709513,\n",
       "  0.02210796915167095,\n",
       "  0.02210796915167095,\n",
       "  0.02210796915167095,\n",
       "  0.02262210796915167,\n",
       "  0.02313624678663239,\n",
       "  0.02313624678663239,\n",
       "  0.02365038560411311,\n",
       "  0.02365038560411311,\n",
       "  0.024678663239074552,\n",
       "  0.024678663239074552,\n",
       "  0.024678663239074552,\n",
       "  0.02519280205655527,\n",
       "  0.02519280205655527,\n",
       "  0.02622107969151671,\n",
       "  0.02673521850899743,\n",
       "  0.028277634961439587,\n",
       "  0.028277634961439587,\n",
       "  0.02930591259640103,\n",
       "  0.02930591259640103,\n",
       "  0.02982005141388175,\n",
       "  0.02982005141388175,\n",
       "  0.030848329048843187,\n",
       "  0.031362467866323906,\n",
       "  0.032390745501285345,\n",
       "  0.033419023136246784,\n",
       "  0.0339331619537275,\n",
       "  0.0339331619537275,\n",
       "  0.03444730077120823,\n",
       "  0.03547557840616967,\n",
       "  0.03547557840616967,\n",
       "  0.03598971722365039,\n",
       "  0.03598971722365039,\n",
       "  0.03650385604113111,\n",
       "  0.03650385604113111,\n",
       "  0.037017994858611826,\n",
       "  0.037532133676092545,\n",
       "  0.0390745501285347,\n",
       "  0.04061696658097686,\n",
       "  0.043187660668380465,\n",
       "  0.0442159383033419,\n",
       "  0.04473007712082262,\n",
       "  0.04473007712082262,\n",
       "  0.04627249357326478,\n",
       "  0.04781491002570694,\n",
       "  0.04832904884318766,\n",
       "  0.049357326478149104,\n",
       "  0.05038560411311054,\n",
       "  0.05089974293059126,\n",
       "  0.05141388174807198,\n",
       "  0.05347043701799486,\n",
       "  0.055012853470437016,\n",
       "  0.055526992287917736,\n",
       "  0.056041131105398455,\n",
       "  0.056041131105398455,\n",
       "  0.056041131105398455,\n",
       "  0.0570694087403599,\n",
       "  0.05758354755784062,\n",
       "  0.05758354755784062,\n",
       "  0.05861182519280206,\n",
       "  0.0596401028277635,\n",
       "  0.060668380462724936,\n",
       "  0.062210796915167094,\n",
       "  0.06323907455012853,\n",
       "  0.06375321336760925,\n",
       "  0.06426735218508997,\n",
       "  0.06478149100257069,\n",
       "  0.06529562982005141,\n",
       "  0.06632390745501285,\n",
       "  0.06632390745501285,\n",
       "  0.06735218508997429,\n",
       "  0.06838046272493574,\n",
       "  0.06940874035989718,\n",
       "  0.07095115681233934,\n",
       "  0.07095115681233934,\n",
       "  0.07197943444730077,\n",
       "  0.0724935732647815,\n",
       "  0.07352185089974293,\n",
       "  0.07403598971722365,\n",
       "  0.07506426735218509,\n",
       "  0.07660668380462725,\n",
       "  0.0781491002570694,\n",
       "  0.07866323907455013,\n",
       "  0.07969151670951156,\n",
       "  0.08020565552699228,\n",
       "  0.080719794344473,\n",
       "  0.08174807197943444,\n",
       "  0.08174807197943444,\n",
       "  0.08174807197943444,\n",
       "  0.0832904884318766,\n",
       "  0.08380462724935733,\n",
       "  0.08483290488431877,\n",
       "  0.08586118251928021,\n",
       "  0.08740359897172237,\n",
       "  0.08791773778920309,\n",
       "  0.08894601542416453,\n",
       "  0.08997429305912596,\n",
       "  0.09254498714652956,\n",
       "  0.09408740359897172,\n",
       "  0.09460154241645244,\n",
       "  0.09511568123393316,\n",
       "  0.09511568123393316,\n",
       "  0.0961439588688946,\n",
       "  0.09768637532133675,\n",
       "  0.10077120822622108,\n",
       "  0.10282776349614396,\n",
       "  0.10437017994858612,\n",
       "  0.10591259640102828,\n",
       "  0.106426735218509,\n",
       "  0.1089974293059126,\n",
       "  0.10951156812339331,\n",
       "  0.11156812339331619,\n",
       "  0.11259640102827763,\n",
       "  0.11362467866323907,\n",
       "  0.11516709511568124,\n",
       "  0.1167095115681234,\n",
       "  0.11825192802056556,\n",
       "  0.11979434447300771,\n",
       "  0.12133676092544987,\n",
       "  0.12236503856041131,\n",
       "  0.12287917737789203,\n",
       "  0.12442159383033419,\n",
       "  0.12699228791773778,\n",
       "  0.12904884318766066,\n",
       "  0.12904884318766066,\n",
       "  0.13059125964010282,\n",
       "  0.13161953727506426,\n",
       "  0.13470437017994857,\n",
       "  0.13573264781491,\n",
       "  0.1372750642673522,\n",
       "  0.13881748071979436,\n",
       "  0.14035989717223651,\n",
       "  0.1424164524421594,\n",
       "  0.14395886889460155,\n",
       "  0.14447300771208227,\n",
       "  0.14601542416452443,\n",
       "  0.14858611825192802,\n",
       "  0.1506426735218509,\n",
       "  0.15218508997429306,\n",
       "  0.1532133676092545,\n",
       "  0.15526992287917737,\n",
       "  0.15732647814910025,\n",
       "  0.15938303341902313,\n",
       "  0.1609254498714653,\n",
       "  0.16349614395886888,\n",
       "  0.16606683804627248,\n",
       "  0.1686375321336761,\n",
       "  0.1712082262210797,\n",
       "  0.1712082262210797,\n",
       "  0.17223650385604114,\n",
       "  0.17326478149100258,\n",
       "  0.17429305912596402,\n",
       "  0.17480719794344474,\n",
       "  0.17532133676092546,\n",
       "  0.1768637532133676,\n",
       "  0.17737789203084833,\n",
       "  0.17840616966580977,\n",
       "  0.18097686375321337,\n",
       "  0.1820051413881748,\n",
       "  0.18354755784061696,\n",
       "  0.18560411311053984,\n",
       "  0.18663239074550128,\n",
       "  0.19023136246786632,\n",
       "  0.19074550128534704,\n",
       "  0.19383033419023135,\n",
       "  0.19742930591259641,\n",
       "  0.1994858611825193,\n",
       "  0.20102827763496145,\n",
       "  0.20308483290488433,\n",
       "  0.20411311053984577,\n",
       "  0.20616966580976864,\n",
       "  0.20874035989717224,\n",
       "  0.20925449871465296,\n",
       "  0.20976863753213368,\n",
       "  0.21079691516709512,\n",
       "  0.212853470437018,\n",
       "  0.2133676092544987,\n",
       "  0.21696658097686375,\n",
       "  0.21902313624678663,\n",
       "  0.22056555269922878,\n",
       "  0.22210796915167094,\n",
       "  0.22416452442159382,\n",
       "  0.2262210796915167,\n",
       "  0.2282776349614396,\n",
       "  0.2308483290488432,\n",
       "  0.23239074550128536,\n",
       "  0.23547557840616967,\n",
       "  0.23753213367609255,\n",
       "  0.24010282776349615,\n",
       "  0.24215938303341902,\n",
       "  0.24473007712082262,\n",
       "  0.24730077120822622,\n",
       "  0.2493573264781491,\n",
       "  0.2519280205655527,\n",
       "  0.25398457583547557,\n",
       "  0.25604113110539845,\n",
       "  0.25758354755784063,\n",
       "  0.2596401028277635,\n",
       "  0.2627249357326478,\n",
       "  0.26580976863753214,\n",
       "  0.26889460154241646,\n",
       "  0.27403598971722365,\n",
       "  0.27712082262210797,\n",
       "  0.2812339331619537,\n",
       "  0.2863753213367609,\n",
       "  0.29203084832904885,\n",
       "  0.2946015424164524,\n",
       "  0.29768637532133674,\n",
       "  0.30077120822622105,\n",
       "  0.30282776349614393,\n",
       "  0.30745501285347043,\n",
       "  0.3110539845758355,\n",
       "  0.3131105398457584,\n",
       "  0.3161953727506427,\n",
       "  0.319280205655527,\n",
       "  0.32390745501285345,\n",
       "  0.32904884318766064,\n",
       "  0.33213367609254496,\n",
       "  0.33521850899742933,\n",
       "  0.3398457583547558,\n",
       "  0.3429305912596401,\n",
       "  0.3491002570694087,\n",
       "  0.3542416452442159,\n",
       "  0.35989717223650386,\n",
       "  0.3655526992287918,\n",
       "  0.36966580976863755,\n",
       "  0.37326478149100256,\n",
       "  0.3768637532133676,\n",
       "  0.38149100257069407,\n",
       "  0.3876606683804627,\n",
       "  0.39331619537275064,\n",
       "  0.39588688946015427,\n",
       "  0.4005141388174807,\n",
       "  0.4071979434447301,\n",
       "  0.412853470437018,\n",
       "  0.42005141388174805,\n",
       "  0.4251928020565553,\n",
       "  0.4318766066838046,\n",
       "  0.43753213367609256,\n",
       "  0.44318766066838045,\n",
       "  0.4473007712082262,\n",
       "  0.45089974293059126,\n",
       "  0.46272493573264784,\n",
       "  0.4719794344473008,\n",
       "  0.47969151670951155,\n",
       "  0.4838046272493573,\n",
       "  0.49460154241645243,\n",
       "  0.5002570694087404,\n",
       "  0.5059125964010283,\n",
       "  0.513624678663239,\n",
       "  0.5244215938303342,\n",
       "  0.5269922879177378,\n",
       "  0.5311053984575835,\n",
       "  0.5383033419023137,\n",
       "  0.5460154241645244,\n",
       "  0.5573264781491003,\n",
       "  0.5727506426735218,\n",
       "  0.579948586118252,\n",
       "  0.5912596401028277,\n",
       "  0.6066838046272494,\n",
       "  0.6215938303341902,\n",
       "  0.6313624678663239,\n",
       "  0.6437017994858611,\n",
       "  0.6596401028277635,\n",
       "  0.6771208226221079,\n",
       "  0.6904884318766067,\n",
       "  0.7131105398457583,\n",
       "  0.7300771208226221,\n",
       "  0.7475578406169666,\n",
       "  0.7629820051413881,\n",
       "  0.7773778920308483,\n",
       "  0.8087403598971722,\n",
       "  0.8200514138817481,\n",
       "  0.8401028277634961,\n",
       "  0.8560411311053985,\n",
       "  0.8817480719794345,\n",
       "  0.8961439588688946,\n",
       "  0.922879177377892,\n",
       "  0.9480719794344473,\n",
       "  0.9758354755784062,\n",
       "  1.0],\n",
       " [0.03279938977879481,\n",
       "  0.07017543859649122,\n",
       "  0.09687261632341723,\n",
       "  0.11136536994660565,\n",
       "  0.13119755911517925,\n",
       "  0.16552250190694126,\n",
       "  0.1853546910755149,\n",
       "  0.20594965675057209,\n",
       "  0.2303585049580473,\n",
       "  0.2418001525553013,\n",
       "  0.2540045766590389,\n",
       "  0.2654462242562929,\n",
       "  0.2745995423340961,\n",
       "  0.2906178489702517,\n",
       "  0.2997711670480549,\n",
       "  0.3119755911517925,\n",
       "  0.32112890922959575,\n",
       "  0.3333333333333333,\n",
       "  0.34324942791762014,\n",
       "  0.35774218154080856,\n",
       "  0.3684210526315789,\n",
       "  0.37833714721586575,\n",
       "  0.38596491228070173,\n",
       "  0.39359267734553777,\n",
       "  0.40427154843630814,\n",
       "  0.41571319603356216,\n",
       "  0.42181540808543094,\n",
       "  0.4286803966437834,\n",
       "  0.43401983218916856,\n",
       "  0.4439359267734554,\n",
       "  0.448512585812357,\n",
       "  0.4576659038901602,\n",
       "  0.4652936689549962,\n",
       "  0.47215865751334857,\n",
       "  0.4797864225781846,\n",
       "  0.4836003051106026,\n",
       "  0.4889397406559878,\n",
       "  0.4935163996948894,\n",
       "  0.5026697177726926,\n",
       "  0.5087719298245614,\n",
       "  0.513348588863463,\n",
       "  0.5194508009153318,\n",
       "  0.5263157894736842,\n",
       "  0.5316552250190694,\n",
       "  0.5377574370709383,\n",
       "  0.543859649122807,\n",
       "  0.5484363081617086,\n",
       "  0.5522501906941266,\n",
       "  0.5598779557589626,\n",
       "  0.562929061784897,\n",
       "  0.5675057208237986,\n",
       "  0.5743707093821511,\n",
       "  0.5797101449275363,\n",
       "  0.5865751334858886,\n",
       "  0.5919145690312738,\n",
       "  0.5957284515636918,\n",
       "  0.5995423340961098,\n",
       "  0.6018306636155606,\n",
       "  0.6064073226544623,\n",
       "  0.6102212051868803,\n",
       "  0.6178489702517163,\n",
       "  0.6216628527841342,\n",
       "  0.6285278413424866,\n",
       "  0.6323417238749046,\n",
       "  0.6384439359267735,\n",
       "  0.6414950419527079,\n",
       "  0.6460717009916095,\n",
       "  0.6483600305110603,\n",
       "  0.6529366895499619,\n",
       "  0.6575133485888635,\n",
       "  0.6605644546147978,\n",
       "  0.6643783371472158,\n",
       "  0.665903890160183,\n",
       "  0.6712433257055682,\n",
       "  0.6720061022120518,\n",
       "  0.6758199847444699,\n",
       "  0.6811594202898551,\n",
       "  0.6864988558352403,\n",
       "  0.6918382913806255,\n",
       "  0.6964149504195271,\n",
       "  0.6994660564454614,\n",
       "  0.7025171624713958,\n",
       "  0.7078565980167811,\n",
       "  0.7116704805491991,\n",
       "  0.7177726926010679,\n",
       "  0.7231121281464531,\n",
       "  0.7246376811594203,\n",
       "  0.7307398932112891,\n",
       "  0.7322654462242563,\n",
       "  0.7376048817696415,\n",
       "  0.7398932112890922,\n",
       "  0.7437070938215103,\n",
       "  0.7459954233409611,\n",
       "  0.7482837528604119,\n",
       "  0.7520976353928299,\n",
       "  0.7559115179252479,\n",
       "  0.7597254004576659,\n",
       "  0.7620137299771167,\n",
       "  0.7643020594965675,\n",
       "  0.7673531655225019,\n",
       "  0.7696414950419527,\n",
       "  0.7711670480549199,\n",
       "  0.7757437070938215,\n",
       "  0.7795575896262396,\n",
       "  0.782608695652174,\n",
       "  0.7848970251716247,\n",
       "  0.7887109077040427,\n",
       "  0.7909992372234935,\n",
       "  0.7925247902364607,\n",
       "  0.7955758962623951,\n",
       "  0.7963386727688787,\n",
       "  0.7986270022883295,\n",
       "  0.8009153318077803,\n",
       "  0.8039664378337147,\n",
       "  0.8077803203661327,\n",
       "  0.8085430968726163,\n",
       "  0.8100686498855835,\n",
       "  0.813119755911518,\n",
       "  0.8138825324180016,\n",
       "  0.8154080854309688,\n",
       "  0.8154080854309688,\n",
       "  0.8176964149504196,\n",
       "  0.8184591914569032,\n",
       "  0.820747520976354,\n",
       "  0.8268497330282227,\n",
       "  0.8291380625476735,\n",
       "  0.8306636155606407,\n",
       "  0.8329519450800915,\n",
       "  0.8329519450800915,\n",
       "  0.8360030511060259,\n",
       "  0.8375286041189931,\n",
       "  0.8398169336384439,\n",
       "  0.8428680396643783,\n",
       "  0.8451563691838292,\n",
       "  0.84744469870328,\n",
       "  0.8489702517162472,\n",
       "  0.8497330282227308,\n",
       "  0.8497330282227308,\n",
       "  0.851258581235698,\n",
       "  0.8543096872616324,\n",
       "  0.8581235697940504,\n",
       "  0.858886346300534,\n",
       "  0.8604118993135011,\n",
       "  0.8619374523264683,\n",
       "  0.8627002288329519,\n",
       "  0.8642257818459191,\n",
       "  0.8657513348588863,\n",
       "  0.8672768878718535,\n",
       "  0.8695652173913043,\n",
       "  0.8703279938977879,\n",
       "  0.8718535469107551,\n",
       "  0.8741418764302059,\n",
       "  0.8749046529366895,\n",
       "  0.8771929824561403,\n",
       "  0.8779557589626239,\n",
       "  0.8802440884820748,\n",
       "  0.881769641495042,\n",
       "  0.881769641495042,\n",
       "  0.881769641495042,\n",
       "  0.8825324180015256,\n",
       "  0.88558352402746,\n",
       "  0.8863463005339436,\n",
       "  0.8878718535469108,\n",
       "  0.8878718535469108,\n",
       "  0.889397406559878,\n",
       "  0.8901601830663616,\n",
       "  0.8909229595728452,\n",
       "  0.8939740655987796,\n",
       "  0.8954996186117468,\n",
       "  0.8962623951182304,\n",
       "  0.897025171624714,\n",
       "  0.8977879481311976,\n",
       "  0.9000762776506483,\n",
       "  0.9016018306636155,\n",
       "  0.9031273836765827,\n",
       "  0.9031273836765827,\n",
       "  0.9038901601830663,\n",
       "  0.9054157131960335,\n",
       "  0.9069412662090007,\n",
       "  0.9084668192219679,\n",
       "  0.9092295957284515,\n",
       "  0.9099923722349351,\n",
       "  0.9099923722349351,\n",
       "  0.9122807017543859,\n",
       "  0.9130434782608695,\n",
       "  0.9145690312738368,\n",
       "  0.916094584286804,\n",
       "  0.9176201372997712,\n",
       "  0.9191456903127384,\n",
       "  0.919908466819222,\n",
       "  0.919908466819222,\n",
       "  0.9206712433257056,\n",
       "  0.9214340198321892,\n",
       "  0.9221967963386728,\n",
       "  0.9221967963386728,\n",
       "  0.92372234935164,\n",
       "  0.9244851258581236,\n",
       "  0.9252479023646072,\n",
       "  0.9252479023646072,\n",
       "  0.9260106788710908,\n",
       "  0.9260106788710908,\n",
       "  0.9267734553775744,\n",
       "  0.927536231884058,\n",
       "  0.9282990083905416,\n",
       "  0.9282990083905416,\n",
       "  0.9282990083905416,\n",
       "  0.9298245614035088,\n",
       "  0.9305873379099924,\n",
       "  0.9336384439359268,\n",
       "  0.9344012204424104,\n",
       "  0.9359267734553776,\n",
       "  0.9366895499618612,\n",
       "  0.938977879481312,\n",
       "  0.9397406559877955,\n",
       "  0.9397406559877955,\n",
       "  0.9405034324942791,\n",
       "  0.9427917620137299,\n",
       "  0.9443173150266971,\n",
       "  0.9450800915331807,\n",
       "  0.9458428680396643,\n",
       "  0.9466056445461479,\n",
       "  0.9466056445461479,\n",
       "  0.9466056445461479,\n",
       "  0.9473684210526315,\n",
       "  0.9481311975591151,\n",
       "  0.9481311975591151,\n",
       "  0.9481311975591151,\n",
       "  0.9481311975591151,\n",
       "  0.9496567505720824,\n",
       "  0.9511823035850496,\n",
       "  0.9527078565980168,\n",
       "  0.9534706331045004,\n",
       "  0.954233409610984,\n",
       "  0.9549961861174676,\n",
       "  0.9557589626239512,\n",
       "  0.9557589626239512,\n",
       "  0.9565217391304348,\n",
       "  0.9565217391304348,\n",
       "  0.9565217391304348,\n",
       "  0.9572845156369184,\n",
       "  0.9572845156369184,\n",
       "  0.958047292143402,\n",
       "  0.958047292143402,\n",
       "  0.9595728451563692,\n",
       "  0.9603356216628528,\n",
       "  0.9603356216628528,\n",
       "  0.9603356216628528,\n",
       "  0.9603356216628528,\n",
       "  0.9603356216628528,\n",
       "  0.9603356216628528,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9610983981693364,\n",
       "  0.9626239511823036,\n",
       "  0.9633867276887872,\n",
       "  0.9633867276887872,\n",
       "  0.9641495041952708,\n",
       "  0.9641495041952708,\n",
       "  0.9641495041952708,\n",
       "  0.9649122807017544,\n",
       "  0.9649122807017544,\n",
       "  0.9664378337147216,\n",
       "  0.9679633867276888,\n",
       "  0.9679633867276888,\n",
       "  0.9679633867276888,\n",
       "  0.9687261632341724,\n",
       "  0.969488939740656,\n",
       "  0.969488939740656,\n",
       "  0.969488939740656,\n",
       "  0.969488939740656,\n",
       "  0.9702517162471396,\n",
       "  0.9702517162471396,\n",
       "  0.9710144927536232,\n",
       "  0.9717772692601068,\n",
       "  0.9717772692601068,\n",
       "  0.9740655987795576,\n",
       "  0.9740655987795576,\n",
       "  0.9740655987795576,\n",
       "  0.9740655987795576,\n",
       "  0.9748283752860412,\n",
       "  0.9748283752860412,\n",
       "  0.9748283752860412,\n",
       "  0.9763539282990084,\n",
       "  0.9763539282990084,\n",
       "  0.977116704805492,\n",
       "  0.9778794813119756,\n",
       "  0.9786422578184591,\n",
       "  0.9794050343249427,\n",
       "  0.9809305873379099,\n",
       "  0.9809305873379099,\n",
       "  0.9809305873379099,\n",
       "  0.9816933638443935,\n",
       "  0.9824561403508771,\n",
       "  0.9824561403508771,\n",
       "  0.9824561403508771,\n",
       "  0.9824561403508771,\n",
       "  0.9824561403508771,\n",
       "  0.9832189168573608,\n",
       "  0.9839816933638444,\n",
       "  0.984744469870328,\n",
       "  0.9855072463768116,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9862700228832952,\n",
       "  0.9877955758962624,\n",
       "  0.9877955758962624,\n",
       "  0.9877955758962624,\n",
       "  0.9877955758962624,\n",
       "  0.9877955758962624,\n",
       "  0.988558352402746,\n",
       "  0.9893211289092296,\n",
       "  0.9893211289092296,\n",
       "  0.9893211289092296,\n",
       "  0.9900839054157132,\n",
       "  0.9900839054157132,\n",
       "  0.9900839054157132,\n",
       "  0.9908466819221968,\n",
       "  0.9908466819221968,\n",
       "  0.9908466819221968,\n",
       "  0.9916094584286804,\n",
       "  0.9916094584286804,\n",
       "  0.9916094584286804,\n",
       "  0.9916094584286804,\n",
       "  0.992372234935164,\n",
       "  0.992372234935164,\n",
       "  0.992372234935164,\n",
       "  0.9931350114416476,\n",
       "  0.9938977879481312,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9946605644546148,\n",
       "  0.9954233409610984,\n",
       "  0.9954233409610984,\n",
       "  0.9954233409610984,\n",
       "  0.9954233409610984,\n",
       "  0.996186117467582,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9969488939740656,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9977116704805492,\n",
       "  0.9984744469870328,\n",
       "  0.9992372234935164,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5515f6-b6b2-4f5a-bb79-d54366c8b1a2",
   "metadata": {},
   "source": [
    "### Model Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd376280-0b1f-437a-b266-e08fb56cd8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8862/2282998648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleader_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleader_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "leader_model = dl\n",
    "explanation = leader_model.explain(test)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8b95b-2352-41d2-a485-70bbd21ac2cb",
   "metadata": {},
   "source": [
    "### Model Summery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fcb32b7-f449-4788-8ca1-6c69f2ccd813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     layer  units    type dropout   l1   l2 mean_rate  rate_rms momentum  \\\n",
      "0        1    608   Input     0.0                                          \n",
      "1        2     50    Tanh     0.0  0.0  0.0  0.048598  0.068596      0.0   \n",
      "2        3     50    Tanh     0.0  0.0  0.0  0.016867  0.034805      0.0   \n",
      "3        4      1  Linear          0.0  0.0  0.000315  0.000093      0.0   \n",
      "\n",
      "  mean_weight weight_rms mean_bias  bias_rms  \n",
      "0                                             \n",
      "1   -0.004561   0.111666  0.002829  0.154115  \n",
      "2    0.009713   0.201611 -0.107674  0.380264  \n",
      "3    0.077396   0.270082  0.433407       0.0  \n"
     ]
    }
   ],
   "source": [
    "summary = dl.summary()\n",
    "print(summary.as_data_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b17f3e2-722f-4a7b-acef-a01f494cba84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2ODeepLearningEstimator : Deep Learning\n",
       "Model Key: DeepLearning_model_python_1684112569648_7\n",
       "</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Status of Neuron Layers: predicting died, regression, tweedie distribution, Automatic loss, 33,051 weights/biases, 492.5 KB, 160,850 training samples, mini-batch size 1</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>layer</th>\n",
       "<th>units</th>\n",
       "<th>type</th>\n",
       "<th>dropout</th>\n",
       "<th>l1</th>\n",
       "<th>l2</th>\n",
       "<th>mean_rate</th>\n",
       "<th>rate_rms</th>\n",
       "<th>momentum</th>\n",
       "<th>mean_weight</th>\n",
       "<th>weight_rms</th>\n",
       "<th>mean_bias</th>\n",
       "<th>bias_rms</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>1</td>\n",
       "<td>608</td>\n",
       "<td>Input</td>\n",
       "<td>0.0</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>50</td>\n",
       "<td>Tanh</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0485976</td>\n",
       "<td>0.0685958</td>\n",
       "<td>0.0</td>\n",
       "<td>-0.0045610</td>\n",
       "<td>0.1116657</td>\n",
       "<td>0.0028287</td>\n",
       "<td>0.1541151</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>50</td>\n",
       "<td>Tanh</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0168667</td>\n",
       "<td>0.0348054</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0097132</td>\n",
       "<td>0.2016115</td>\n",
       "<td>-0.1076740</td>\n",
       "<td>0.3802640</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>1</td>\n",
       "<td>Linear</td>\n",
       "<td></td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0003145</td>\n",
       "<td>0.0000925</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0773962</td>\n",
       "<td>0.2700819</td>\n",
       "<td>0.4334068</td>\n",
       "<td>0.0000000</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegression: deeplearning\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.1318906315771173\n",
       "RMSE: 0.3631674979635668\n",
       "MAE: 0.24957397881277027\n",
       "RMSLE: 0.2391310738918054\n",
       "Mean Residual Deviance: 1.146853603868233</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegression: deeplearning\n",
       "** Reported on validation data. **\n",
       "\n",
       "MSE: 0.14326793567130822\n",
       "RMSE: 0.3785075107198115\n",
       "MAE: 0.253226398777981\n",
       "RMSLE: 0.241239172235864\n",
       "Mean Residual Deviance: 1.159262350563437</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-6.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-6 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-6 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-6 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table th,\n",
       "#h2o-table-6 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Scoring History: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>timestamp</th>\n",
       "<th>duration</th>\n",
       "<th>training_speed</th>\n",
       "<th>epochs</th>\n",
       "<th>iterations</th>\n",
       "<th>samples</th>\n",
       "<th>training_rmse</th>\n",
       "<th>training_deviance</th>\n",
       "<th>training_mae</th>\n",
       "<th>training_r2</th>\n",
       "<th>validation_rmse</th>\n",
       "<th>validation_deviance</th>\n",
       "<th>validation_mae</th>\n",
       "<th>validation_r2</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>2023-05-14 20:24:57</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>None</td>\n",
       "<td>0.0</td>\n",
       "<td>0</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:25:18</td>\n",
       "<td>21.732 sec</td>\n",
       "<td>787 obs/sec</td>\n",
       "<td>1.0</td>\n",
       "<td>1</td>\n",
       "<td>16085.0</td>\n",
       "<td>0.3901091</td>\n",
       "<td>1.3866745</td>\n",
       "<td>0.3037826</td>\n",
       "<td>0.3658953</td>\n",
       "<td>0.3904106</td>\n",
       "<td>1.3773798</td>\n",
       "<td>0.3037035</td>\n",
       "<td>0.3701105</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:25:39</td>\n",
       "<td>42.455 sec</td>\n",
       "<td>803 obs/sec</td>\n",
       "<td>2.0</td>\n",
       "<td>2</td>\n",
       "<td>32170.0</td>\n",
       "<td>0.3819092</td>\n",
       "<td>1.3274833</td>\n",
       "<td>0.2843706</td>\n",
       "<td>0.3922723</td>\n",
       "<td>0.3882288</td>\n",
       "<td>1.3388795</td>\n",
       "<td>0.2894434</td>\n",
       "<td>0.3771310</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:25:57</td>\n",
       "<td> 1 min  0.444 sec</td>\n",
       "<td>849 obs/sec</td>\n",
       "<td>3.0</td>\n",
       "<td>3</td>\n",
       "<td>48255.0</td>\n",
       "<td>0.3718268</td>\n",
       "<td>1.3084362</td>\n",
       "<td>0.2767869</td>\n",
       "<td>0.4239369</td>\n",
       "<td>0.3678588</td>\n",
       "<td>1.3082037</td>\n",
       "<td>0.2748648</td>\n",
       "<td>0.4407788</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:26:17</td>\n",
       "<td> 1 min 20.388 sec</td>\n",
       "<td>850 obs/sec</td>\n",
       "<td>4.0</td>\n",
       "<td>4</td>\n",
       "<td>64340.0</td>\n",
       "<td>0.3835893</td>\n",
       "<td>1.3168401</td>\n",
       "<td>0.3012460</td>\n",
       "<td>0.3869136</td>\n",
       "<td>0.3770329</td>\n",
       "<td>1.2872033</td>\n",
       "<td>0.3004748</td>\n",
       "<td>0.4125379</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:26:37</td>\n",
       "<td> 1 min 40.654 sec</td>\n",
       "<td>849 obs/sec</td>\n",
       "<td>5.0</td>\n",
       "<td>5</td>\n",
       "<td>80425.0</td>\n",
       "<td>0.3595484</td>\n",
       "<td>1.2824804</td>\n",
       "<td>0.2815555</td>\n",
       "<td>0.4613539</td>\n",
       "<td>0.3569553</td>\n",
       "<td>1.2528730</td>\n",
       "<td>0.2796601</td>\n",
       "<td>0.4734388</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:26:58</td>\n",
       "<td> 2 min  1.879 sec</td>\n",
       "<td>841 obs/sec</td>\n",
       "<td>6.0</td>\n",
       "<td>6</td>\n",
       "<td>96510.0</td>\n",
       "<td>0.3678048</td>\n",
       "<td>1.2320424</td>\n",
       "<td>0.2633062</td>\n",
       "<td>0.4363317</td>\n",
       "<td>0.3599152</td>\n",
       "<td>1.2059594</td>\n",
       "<td>0.2584567</td>\n",
       "<td>0.4646700</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:27:19</td>\n",
       "<td> 2 min 23.673 sec</td>\n",
       "<td>835 obs/sec</td>\n",
       "<td>7.0</td>\n",
       "<td>7</td>\n",
       "<td>112595.0</td>\n",
       "<td>0.3628096</td>\n",
       "<td>1.2192485</td>\n",
       "<td>0.2689372</td>\n",
       "<td>0.4515382</td>\n",
       "<td>0.3648935</td>\n",
       "<td>1.2316138</td>\n",
       "<td>0.2710325</td>\n",
       "<td>0.4497582</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:27:39</td>\n",
       "<td> 2 min 43.369 sec</td>\n",
       "<td>840 obs/sec</td>\n",
       "<td>8.0</td>\n",
       "<td>8</td>\n",
       "<td>128680.0</td>\n",
       "<td>0.3904591</td>\n",
       "<td>1.2173947</td>\n",
       "<td>0.2625226</td>\n",
       "<td>0.3647570</td>\n",
       "<td>0.3867844</td>\n",
       "<td>1.2473614</td>\n",
       "<td>0.2624530</td>\n",
       "<td>0.3817569</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:27:58</td>\n",
       "<td> 3 min  1.729 sec</td>\n",
       "<td>850 obs/sec</td>\n",
       "<td>9.0</td>\n",
       "<td>9</td>\n",
       "<td>144765.0</td>\n",
       "<td>0.3476969</td>\n",
       "<td>1.1655005</td>\n",
       "<td>0.2551039</td>\n",
       "<td>0.4962785</td>\n",
       "<td>0.3441392</td>\n",
       "<td>1.1659491</td>\n",
       "<td>0.2520042</td>\n",
       "<td>0.5105712</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-05-14 20:28:19</td>\n",
       "<td> 3 min 22.631 sec</td>\n",
       "<td>846 obs/sec</td>\n",
       "<td>10.0</td>\n",
       "<td>10</td>\n",
       "<td>160850.0</td>\n",
       "<td>0.3631675</td>\n",
       "<td>1.1468536</td>\n",
       "<td>0.2495740</td>\n",
       "<td>0.4504557</td>\n",
       "<td>0.3785075</td>\n",
       "<td>1.1592624</td>\n",
       "<td>0.2532264</td>\n",
       "<td>0.4079338</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-7.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-7 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-7 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-7 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table th,\n",
       "#h2o-table-7 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Variable Importances: </caption>\n",
       "    <thead><tr><th>variable</th>\n",
       "<th>relative_importance</th>\n",
       "<th>scaled_importance</th>\n",
       "<th>percentage</th></tr></thead>\n",
       "    <tbody><tr><td>GEN002</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0061584</td></tr>\n",
       "<tr><td>HEP002</td>\n",
       "<td>0.8626058</td>\n",
       "<td>0.8626058</td>\n",
       "<td>0.0053123</td></tr>\n",
       "<tr><td>INF002</td>\n",
       "<td>0.8485610</td>\n",
       "<td>0.8485610</td>\n",
       "<td>0.0052258</td></tr>\n",
       "<tr><td>XXX000</td>\n",
       "<td>0.8024213</td>\n",
       "<td>0.8024213</td>\n",
       "<td>0.0049416</td></tr>\n",
       "<tr><td>HEP008</td>\n",
       "<td>0.7265630</td>\n",
       "<td>0.7265630</td>\n",
       "<td>0.0044745</td></tr>\n",
       "<tr><td>NEO070</td>\n",
       "<td>0.7262210</td>\n",
       "<td>0.7262210</td>\n",
       "<td>0.0044723</td></tr>\n",
       "<tr><td>CIR018</td>\n",
       "<td>0.7152753</td>\n",
       "<td>0.7152753</td>\n",
       "<td>0.0044049</td></tr>\n",
       "<tr><td>HEP005</td>\n",
       "<td>0.7089593</td>\n",
       "<td>0.7089593</td>\n",
       "<td>0.0043660</td></tr>\n",
       "<tr><td>END011</td>\n",
       "<td>0.6037503</td>\n",
       "<td>0.6037503</td>\n",
       "<td>0.0037181</td></tr>\n",
       "<tr><td>NVS020</td>\n",
       "<td>0.6030979</td>\n",
       "<td>0.6030979</td>\n",
       "<td>0.0037141</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>NEO015</td>\n",
       "<td>0.1567791</td>\n",
       "<td>0.1567791</td>\n",
       "<td>0.0009655</td></tr>\n",
       "<tr><td>NVS006</td>\n",
       "<td>0.1564519</td>\n",
       "<td>0.1564519</td>\n",
       "<td>0.0009635</td></tr>\n",
       "<tr><td>INJ026</td>\n",
       "<td>0.1539459</td>\n",
       "<td>0.1539459</td>\n",
       "<td>0.0009481</td></tr>\n",
       "<tr><td>NEO039</td>\n",
       "<td>0.1521906</td>\n",
       "<td>0.1521906</td>\n",
       "<td>0.0009372</td></tr>\n",
       "<tr><td>RSP017</td>\n",
       "<td>0.1521614</td>\n",
       "<td>0.1521614</td>\n",
       "<td>0.0009371</td></tr>\n",
       "<tr><td>BLD002</td>\n",
       "<td>0.1520947</td>\n",
       "<td>0.1520947</td>\n",
       "<td>0.0009367</td></tr>\n",
       "<tr><td>RSP014</td>\n",
       "<td>0.1476944</td>\n",
       "<td>0.1476944</td>\n",
       "<td>0.0009096</td></tr>\n",
       "<tr><td>CIR032</td>\n",
       "<td>0.1476644</td>\n",
       "<td>0.1476644</td>\n",
       "<td>0.0009094</td></tr>\n",
       "<tr><td>INJ017</td>\n",
       "<td>0.1476041</td>\n",
       "<td>0.1476041</td>\n",
       "<td>0.0009090</td></tr>\n",
       "<tr><td>DIG019</td>\n",
       "<td>0.1468766</td>\n",
       "<td>0.1468766</td>\n",
       "<td>0.0009045</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "<pre style='font-size: smaller; margin-bottom: 1em;'>[608 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2ODeepLearningEstimator : Deep Learning\n",
       "Model Key: DeepLearning_model_python_1684112569648_7\n",
       "\n",
       "\n",
       "Status of Neuron Layers: predicting died, regression, tweedie distribution, Automatic loss, 33,051 weights/biases, 492.5 KB, 160,850 training samples, mini-batch size 1\n",
       "    layer    units    type    dropout    l1    l2    mean_rate               rate_rms               momentum    mean_weight            weight_rms           mean_bias             bias_rms\n",
       "--  -------  -------  ------  ---------  ----  ----  ----------------------  ---------------------  ----------  ---------------------  -------------------  --------------------  -----------------------\n",
       "    1        608      Input   0.0\n",
       "    2        50       Tanh    0.0        0.0   0.0   0.04859755996400508     0.06859582662582397    0.0         -0.004561034837546376  0.11166572570800781  0.00282871958451425   0.15411514043807983\n",
       "    3        50       Tanh    0.0        0.0   0.0   0.0168667189411819      0.03480544686317444    0.0         0.009713222266291267   0.2016114592552185   -0.10767395355013538  0.3802640438079834\n",
       "    4        1        Linear             0.0   0.0   0.00031452912866370753  9.250076254829764e-05  0.0         0.07739621295942925    0.27008187770843506  0.43340676828323865   1.0971281125650402e-154\n",
       "\n",
       "ModelMetricsRegression: deeplearning\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.1318906315771173\n",
       "RMSE: 0.3631674979635668\n",
       "MAE: 0.24957397881277027\n",
       "RMSLE: 0.2391310738918054\n",
       "Mean Residual Deviance: 1.146853603868233\n",
       "\n",
       "ModelMetricsRegression: deeplearning\n",
       "** Reported on validation data. **\n",
       "\n",
       "MSE: 0.14326793567130822\n",
       "RMSE: 0.3785075107198115\n",
       "MAE: 0.253226398777981\n",
       "RMSLE: 0.241239172235864\n",
       "Mean Residual Deviance: 1.159262350563437\n",
       "\n",
       "Scoring History: \n",
       "    timestamp            duration          training_speed    epochs    iterations    samples    training_rmse    training_deviance    training_mae    training_r2    validation_rmse    validation_deviance    validation_mae    validation_r2\n",
       "--  -------------------  ----------------  ----------------  --------  ------------  ---------  ---------------  -------------------  --------------  -------------  -----------------  ---------------------  ----------------  ---------------\n",
       "    2023-05-14 20:24:57  0.000 sec                           0         0             0          nan              nan                  nan             nan            nan                nan                    nan               nan\n",
       "    2023-05-14 20:25:18  21.732 sec        787 obs/sec       1         1             16085      0.390109         1.38667              0.303783        0.365895       0.390411           1.37738                0.303703          0.37011\n",
       "    2023-05-14 20:25:39  42.455 sec        803 obs/sec       2         2             32170      0.381909         1.32748              0.284371        0.392272       0.388229           1.33888                0.289443          0.377131\n",
       "    2023-05-14 20:25:57  1 min  0.444 sec  849 obs/sec       3         3             48255      0.371827         1.30844              0.276787        0.423937       0.367859           1.3082                 0.274865          0.440779\n",
       "    2023-05-14 20:26:17  1 min 20.388 sec  850 obs/sec       4         4             64340      0.383589         1.31684              0.301246        0.386914       0.377033           1.2872                 0.300475          0.412538\n",
       "    2023-05-14 20:26:37  1 min 40.654 sec  849 obs/sec       5         5             80425      0.359548         1.28248              0.281556        0.461354       0.356955           1.25287                0.27966           0.473439\n",
       "    2023-05-14 20:26:58  2 min  1.879 sec  841 obs/sec       6         6             96510      0.367805         1.23204              0.263306        0.436332       0.359915           1.20596                0.258457          0.46467\n",
       "    2023-05-14 20:27:19  2 min 23.673 sec  835 obs/sec       7         7             112595     0.36281          1.21925              0.268937        0.451538       0.364894           1.23161                0.271032          0.449758\n",
       "    2023-05-14 20:27:39  2 min 43.369 sec  840 obs/sec       8         8             128680     0.390459         1.21739              0.262523        0.364757       0.386784           1.24736                0.262453          0.381757\n",
       "    2023-05-14 20:27:58  3 min  1.729 sec  850 obs/sec       9         9             144765     0.347697         1.1655               0.255104        0.496279       0.344139           1.16595                0.252004          0.510571\n",
       "    2023-05-14 20:28:19  3 min 22.631 sec  846 obs/sec       10        10            160850     0.363167         1.14685              0.249574        0.450456       0.378508           1.15926                0.253226          0.407934\n",
       "\n",
       "Variable Importances: \n",
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ---------------------\n",
       "GEN002      1.0                    1.0                  0.006158384677611867\n",
       "HEP002      0.8626057505607605     0.8626057505607605   0.005312258037073272\n",
       "INF002      0.8485609889030457     0.8485609889030457   0.00522576499207969\n",
       "XXX000      0.8024213314056396     0.8024213314056396   0.004941619232317405\n",
       "HEP008      0.7265629768371582     0.7265629768371582   0.004474454303874021\n",
       "NEO070      0.726220965385437      0.726220965385437    0.004472348065790174\n",
       "CIR018      0.7152753472328186     0.7152753472328186   0.004404940738672098\n",
       "HEP005      0.7089593410491943     0.7089593410491943   0.0043660443429671646\n",
       "END011      0.6037503480911255     0.6037503480911255   0.0037181268927872183\n",
       "NVS020      0.6030978560447693     0.6030978560447693   0.003714108595766675\n",
       "---         ---                    ---                  ---\n",
       "NEO015      0.15677909553050995    0.15677909553050995  0.0009655059796849396\n",
       "NVS006      0.15645194053649902    0.15645194053649902  0.0009634912333826186\n",
       "INJ026      0.1539458930492401     0.1539458930492401   0.0009480580289357156\n",
       "NEO039      0.15219064056873322    0.15219064056873322  0.0009372485089544217\n",
       "RSP017      0.1521613746881485     0.1521613746881485   0.0009370682784038519\n",
       "BLD002      0.1520947366952896     0.1520947366952896   0.0009366578960096829\n",
       "RSP014      0.1476944386959076     0.1476944386959076   0.0009095591682333626\n",
       "CIR032      0.1476643681526184     0.1476643681526184   0.000909373982260323\n",
       "INJ017      0.1476040929555893     0.1476040929555893   0.0009090027844104988\n",
       "DIG019      0.14687658846378326    0.14687658846378326  0.0009045225318952668\n",
       "[608 rows x 4 columns]\n",
       "\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812b8ab0-c953-4078-b701-a2ea4665c5b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8862/1682044013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "dl.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80875168-4520-4bcf-906d-5936ed7a3c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': 3.4028235e+38, 'actual': 3.4028235e+38, 'input': 3.4028235e+38}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.params['max_w2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811d942-2cb2-4845-9075-7b20290eca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Amirsajjad",
   "language": "python",
   "name": "amirsajjad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
